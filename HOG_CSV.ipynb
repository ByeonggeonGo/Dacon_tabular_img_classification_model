{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c rapidsai -c nvidia -c conda-forge dask-cuda cudatoolkit=11.6\n",
    "# nvidia-smi\n",
    "# conda install -c menpo opencv\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import dask.dataframe as dd\n",
    "import dask.multiprocessing\n",
    "import dask\n",
    "import numpy as np\n",
    "from dask import delayed\n",
    "import dask_image.imread\n",
    "import dask_image.ndfilters\n",
    "import dask_image.ndmeasure\n",
    "import cv2\n",
    "from glob import glob\n",
    "import json\n",
    "import dask.array as da\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "from dask import compute\n",
    "from sklearn.metrics import f1_score\n",
    "# f1_score(y_true, y_pred, average=[‘micro’, ‘macro’, ‘samples’,’weighted’ 중 하나 선택])\n",
    "# import tensorflow as tf\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import tensorflow as tf\n",
    "\n",
    "path = os.getcwd()\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# # client.close()\n",
    "# cluster = LocalCluster()\n",
    "# client = Client(cluster)\n",
    "# print(cluster,client)\n",
    "# client.close()\n",
    "# http://localhost:8787/status\n",
    "\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "# from dask.distributed import Client\n",
    "# client.close()\n",
    "# # http://127.0.0.1:34497/status\n",
    "# cluster = LocalCUDACluster()\n",
    "# client = Client(cluster)\n",
    "# print(cluster,client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'0':'정상','1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "\n",
    "#ensemble_labler\n",
    "label_description = {}\n",
    "for key, value in disease.items():\n",
    "    label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
    "    for disease_code in value:\n",
    "        for risk_code in risk:\n",
    "            label = f'{key}_{disease_code}_{risk_code}'\n",
    "            label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
    "\n",
    "# label_description = {\n",
    "# \"1_00_0\" : \"딸기\", \n",
    "# \"2_00_0\" : \"토마토\",\n",
    "# \"2_a5_2\" : \"토마토_흰가루병_중기\",\n",
    "# \"3_00_0\" : \"파프리카\",\n",
    "# \"3_a9_1\" : \"파프리카_흰가루병_초기\",\n",
    "# \"3_a9_2\" : \"파프리카_흰가루병_중기\",\n",
    "# \"3_a9_3\" : \"파프리카_흰가루병_말기\",\n",
    "# \"3_b3_1\" : \"파프리카_칼슘결핍_초기\",\n",
    "# \"3_b6_1\" : \"파프리카_다량원소결필(N)_초기\",\n",
    "# \"3_b7_1\" : \"파프리카_다량원소결필(P)_초기\",\n",
    "# \"3_b8_1\" : \"파프리카_다량원소결필(K)_초기\",\n",
    "# \"4_00_0\" : \"오이\",\n",
    "# \"5_00_0\" : \"고추\",\n",
    "#  \"5_a7_2\" : \"고추_탄저병_중기\",\n",
    "#  \"5_b6_1\" : \"고추_다량원소결필(N)_초기\",\n",
    "# \"5_b7_1\" : \"고추_다량원소결필(P)_초기\",\n",
    "#  \"5_b8_1\" : \"고추_다량원소결필(K)_초기\",\n",
    "# \"6_00_0\" : \"시설포도\",\n",
    "# \"6_a11_1\" : \"시설포도_탄저병_초기\",\n",
    "#  \"6_a11_2\" : \"시설포도_탄저병_중기\",\n",
    "#  \"6_a12_1\" : \"시설포도_노균병_초기\",\n",
    "# \"6_a12_2\" : \"시설포도_노균병_중기\",\n",
    "#  \"6_b4_1\" : \"시설포도_일소피해_초기\",\n",
    "#  \"6_b4_3\" : \"시설포도_일소피해_말기\",\n",
    "# \"6_b5_1\" : \"시설포도_축과병_초기\"   }\n",
    "\n",
    "\n",
    "global ensemble_label_encoder\n",
    "ensemble_label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "ensemble_label_decoder = {val:key for key, val in ensemble_label_encoder.items()}\n",
    "\n",
    "#crop_labler\n",
    "crop_label_description = {}\n",
    "for key, value in disease.items():\n",
    "    crop_label_description[f'{key}'] = f'{crop[key]}'\n",
    "    \n",
    "global crop_label_encoder\n",
    "crop_label_encoder = {key:idx for idx, key in enumerate(crop_label_description)}\n",
    "crop_label_decoder = {val:key for key, val in crop_label_encoder.items()}\n",
    "\n",
    "\n",
    "#disease_labler\n",
    "disease_label_description = {}\n",
    "for key, value in disease.items():\n",
    "    disease_label_description[f'00'] = \"정상\"\n",
    "    for disease_code,value in value.items():\n",
    "        label = f'{disease_code}'\n",
    "        disease_label_description[label] = f'{value}'\n",
    "\n",
    "global disease_label_encoder\n",
    "disease_label_encoder = {key:idx for idx, key in enumerate(disease_label_description)}\n",
    "disease_label_decoder = {val:key for key, val in disease_label_encoder.items()}\n",
    "\n",
    "\n",
    "#risk_labler\n",
    "risk_label_description = {}\n",
    "for key, value in risk.items():\n",
    "    label = f'{key}'\n",
    "    risk_label_description[label] = f'{value}'\n",
    "\n",
    "global risk_label_encoder\n",
    "risk_label_encoder = {key:idx for idx, key in enumerate(risk_label_description)}\n",
    "risk_label_decoder = {val:key for key, val in risk_label_encoder.items()}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': '딸기', '2': '토마토', '3': '파프리카', '4': '오이', '5': '고추', '6': '시설포도'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_label_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': '정상', '1': '초기', '2': '중기', '3': '말기'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_label_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00': '정상',\n",
       " 'a1': '딸기잿빛곰팡이병',\n",
       " 'a2': '딸기흰가루병',\n",
       " 'b1': '냉해피해',\n",
       " 'b6': '다량원소결핍 (N)',\n",
       " 'b7': '다량원소결핍 (P)',\n",
       " 'b8': '다량원소결핍 (K)',\n",
       " 'a5': '토마토흰가루병',\n",
       " 'a6': '토마토잿빛곰팡이병',\n",
       " 'b2': '열과',\n",
       " 'b3': '칼슘결핍',\n",
       " 'a9': '파프리카흰가루병',\n",
       " 'a10': '파프리카잘록병',\n",
       " 'a3': '오이노균병',\n",
       " 'a4': '오이흰가루병',\n",
       " 'a7': '고추탄저병',\n",
       " 'a8': '고추흰가루병',\n",
       " 'a11': '시설포도탄저병',\n",
       " 'a12': '시설포도노균병',\n",
       " 'b4': '일소피해',\n",
       " 'b5': '축과병'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disease_label_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_00_0': '딸기_정상',\n",
       " '1_a1_0': '딸기_딸기잿빛곰팡이병_정상',\n",
       " '1_a1_1': '딸기_딸기잿빛곰팡이병_초기',\n",
       " '1_a1_2': '딸기_딸기잿빛곰팡이병_중기',\n",
       " '1_a1_3': '딸기_딸기잿빛곰팡이병_말기',\n",
       " '1_a2_0': '딸기_딸기흰가루병_정상',\n",
       " '1_a2_1': '딸기_딸기흰가루병_초기',\n",
       " '1_a2_2': '딸기_딸기흰가루병_중기',\n",
       " '1_a2_3': '딸기_딸기흰가루병_말기',\n",
       " '1_b1_0': '딸기_냉해피해_정상',\n",
       " '1_b1_1': '딸기_냉해피해_초기',\n",
       " '1_b1_2': '딸기_냉해피해_중기',\n",
       " '1_b1_3': '딸기_냉해피해_말기',\n",
       " '1_b6_0': '딸기_다량원소결핍 (N)_정상',\n",
       " '1_b6_1': '딸기_다량원소결핍 (N)_초기',\n",
       " '1_b6_2': '딸기_다량원소결핍 (N)_중기',\n",
       " '1_b6_3': '딸기_다량원소결핍 (N)_말기',\n",
       " '1_b7_0': '딸기_다량원소결핍 (P)_정상',\n",
       " '1_b7_1': '딸기_다량원소결핍 (P)_초기',\n",
       " '1_b7_2': '딸기_다량원소결핍 (P)_중기',\n",
       " '1_b7_3': '딸기_다량원소결핍 (P)_말기',\n",
       " '1_b8_0': '딸기_다량원소결핍 (K)_정상',\n",
       " '1_b8_1': '딸기_다량원소결핍 (K)_초기',\n",
       " '1_b8_2': '딸기_다량원소결핍 (K)_중기',\n",
       " '1_b8_3': '딸기_다량원소결핍 (K)_말기',\n",
       " '2_00_0': '토마토_정상',\n",
       " '2_a5_0': '토마토_토마토흰가루병_정상',\n",
       " '2_a5_1': '토마토_토마토흰가루병_초기',\n",
       " '2_a5_2': '토마토_토마토흰가루병_중기',\n",
       " '2_a5_3': '토마토_토마토흰가루병_말기',\n",
       " '2_a6_0': '토마토_토마토잿빛곰팡이병_정상',\n",
       " '2_a6_1': '토마토_토마토잿빛곰팡이병_초기',\n",
       " '2_a6_2': '토마토_토마토잿빛곰팡이병_중기',\n",
       " '2_a6_3': '토마토_토마토잿빛곰팡이병_말기',\n",
       " '2_b2_0': '토마토_열과_정상',\n",
       " '2_b2_1': '토마토_열과_초기',\n",
       " '2_b2_2': '토마토_열과_중기',\n",
       " '2_b2_3': '토마토_열과_말기',\n",
       " '2_b3_0': '토마토_칼슘결핍_정상',\n",
       " '2_b3_1': '토마토_칼슘결핍_초기',\n",
       " '2_b3_2': '토마토_칼슘결핍_중기',\n",
       " '2_b3_3': '토마토_칼슘결핍_말기',\n",
       " '2_b6_0': '토마토_다량원소결핍 (N)_정상',\n",
       " '2_b6_1': '토마토_다량원소결핍 (N)_초기',\n",
       " '2_b6_2': '토마토_다량원소결핍 (N)_중기',\n",
       " '2_b6_3': '토마토_다량원소결핍 (N)_말기',\n",
       " '2_b7_0': '토마토_다량원소결핍 (P)_정상',\n",
       " '2_b7_1': '토마토_다량원소결핍 (P)_초기',\n",
       " '2_b7_2': '토마토_다량원소결핍 (P)_중기',\n",
       " '2_b7_3': '토마토_다량원소결핍 (P)_말기',\n",
       " '2_b8_0': '토마토_다량원소결핍 (K)_정상',\n",
       " '2_b8_1': '토마토_다량원소결핍 (K)_초기',\n",
       " '2_b8_2': '토마토_다량원소결핍 (K)_중기',\n",
       " '2_b8_3': '토마토_다량원소결핍 (K)_말기',\n",
       " '3_00_0': '파프리카_정상',\n",
       " '3_a9_0': '파프리카_파프리카흰가루병_정상',\n",
       " '3_a9_1': '파프리카_파프리카흰가루병_초기',\n",
       " '3_a9_2': '파프리카_파프리카흰가루병_중기',\n",
       " '3_a9_3': '파프리카_파프리카흰가루병_말기',\n",
       " '3_a10_0': '파프리카_파프리카잘록병_정상',\n",
       " '3_a10_1': '파프리카_파프리카잘록병_초기',\n",
       " '3_a10_2': '파프리카_파프리카잘록병_중기',\n",
       " '3_a10_3': '파프리카_파프리카잘록병_말기',\n",
       " '3_b3_0': '파프리카_칼슘결핍_정상',\n",
       " '3_b3_1': '파프리카_칼슘결핍_초기',\n",
       " '3_b3_2': '파프리카_칼슘결핍_중기',\n",
       " '3_b3_3': '파프리카_칼슘결핍_말기',\n",
       " '3_b6_0': '파프리카_다량원소결핍 (N)_정상',\n",
       " '3_b6_1': '파프리카_다량원소결핍 (N)_초기',\n",
       " '3_b6_2': '파프리카_다량원소결핍 (N)_중기',\n",
       " '3_b6_3': '파프리카_다량원소결핍 (N)_말기',\n",
       " '3_b7_0': '파프리카_다량원소결핍 (P)_정상',\n",
       " '3_b7_1': '파프리카_다량원소결핍 (P)_초기',\n",
       " '3_b7_2': '파프리카_다량원소결핍 (P)_중기',\n",
       " '3_b7_3': '파프리카_다량원소결핍 (P)_말기',\n",
       " '3_b8_0': '파프리카_다량원소결핍 (K)_정상',\n",
       " '3_b8_1': '파프리카_다량원소결핍 (K)_초기',\n",
       " '3_b8_2': '파프리카_다량원소결핍 (K)_중기',\n",
       " '3_b8_3': '파프리카_다량원소결핍 (K)_말기',\n",
       " '4_00_0': '오이_정상',\n",
       " '4_a3_0': '오이_오이노균병_정상',\n",
       " '4_a3_1': '오이_오이노균병_초기',\n",
       " '4_a3_2': '오이_오이노균병_중기',\n",
       " '4_a3_3': '오이_오이노균병_말기',\n",
       " '4_a4_0': '오이_오이흰가루병_정상',\n",
       " '4_a4_1': '오이_오이흰가루병_초기',\n",
       " '4_a4_2': '오이_오이흰가루병_중기',\n",
       " '4_a4_3': '오이_오이흰가루병_말기',\n",
       " '4_b1_0': '오이_냉해피해_정상',\n",
       " '4_b1_1': '오이_냉해피해_초기',\n",
       " '4_b1_2': '오이_냉해피해_중기',\n",
       " '4_b1_3': '오이_냉해피해_말기',\n",
       " '4_b6_0': '오이_다량원소결핍 (N)_정상',\n",
       " '4_b6_1': '오이_다량원소결핍 (N)_초기',\n",
       " '4_b6_2': '오이_다량원소결핍 (N)_중기',\n",
       " '4_b6_3': '오이_다량원소결핍 (N)_말기',\n",
       " '4_b7_0': '오이_다량원소결핍 (P)_정상',\n",
       " '4_b7_1': '오이_다량원소결핍 (P)_초기',\n",
       " '4_b7_2': '오이_다량원소결핍 (P)_중기',\n",
       " '4_b7_3': '오이_다량원소결핍 (P)_말기',\n",
       " '4_b8_0': '오이_다량원소결핍 (K)_정상',\n",
       " '4_b8_1': '오이_다량원소결핍 (K)_초기',\n",
       " '4_b8_2': '오이_다량원소결핍 (K)_중기',\n",
       " '4_b8_3': '오이_다량원소결핍 (K)_말기',\n",
       " '5_00_0': '고추_정상',\n",
       " '5_a7_0': '고추_고추탄저병_정상',\n",
       " '5_a7_1': '고추_고추탄저병_초기',\n",
       " '5_a7_2': '고추_고추탄저병_중기',\n",
       " '5_a7_3': '고추_고추탄저병_말기',\n",
       " '5_a8_0': '고추_고추흰가루병_정상',\n",
       " '5_a8_1': '고추_고추흰가루병_초기',\n",
       " '5_a8_2': '고추_고추흰가루병_중기',\n",
       " '5_a8_3': '고추_고추흰가루병_말기',\n",
       " '5_b3_0': '고추_칼슘결핍_정상',\n",
       " '5_b3_1': '고추_칼슘결핍_초기',\n",
       " '5_b3_2': '고추_칼슘결핍_중기',\n",
       " '5_b3_3': '고추_칼슘결핍_말기',\n",
       " '5_b6_0': '고추_다량원소결핍 (N)_정상',\n",
       " '5_b6_1': '고추_다량원소결핍 (N)_초기',\n",
       " '5_b6_2': '고추_다량원소결핍 (N)_중기',\n",
       " '5_b6_3': '고추_다량원소결핍 (N)_말기',\n",
       " '5_b7_0': '고추_다량원소결핍 (P)_정상',\n",
       " '5_b7_1': '고추_다량원소결핍 (P)_초기',\n",
       " '5_b7_2': '고추_다량원소결핍 (P)_중기',\n",
       " '5_b7_3': '고추_다량원소결핍 (P)_말기',\n",
       " '5_b8_0': '고추_다량원소결핍 (K)_정상',\n",
       " '5_b8_1': '고추_다량원소결핍 (K)_초기',\n",
       " '5_b8_2': '고추_다량원소결핍 (K)_중기',\n",
       " '5_b8_3': '고추_다량원소결핍 (K)_말기',\n",
       " '6_00_0': '시설포도_정상',\n",
       " '6_a11_0': '시설포도_시설포도탄저병_정상',\n",
       " '6_a11_1': '시설포도_시설포도탄저병_초기',\n",
       " '6_a11_2': '시설포도_시설포도탄저병_중기',\n",
       " '6_a11_3': '시설포도_시설포도탄저병_말기',\n",
       " '6_a12_0': '시설포도_시설포도노균병_정상',\n",
       " '6_a12_1': '시설포도_시설포도노균병_초기',\n",
       " '6_a12_2': '시설포도_시설포도노균병_중기',\n",
       " '6_a12_3': '시설포도_시설포도노균병_말기',\n",
       " '6_b4_0': '시설포도_일소피해_정상',\n",
       " '6_b4_1': '시설포도_일소피해_초기',\n",
       " '6_b4_2': '시설포도_일소피해_중기',\n",
       " '6_b4_3': '시설포도_일소피해_말기',\n",
       " '6_b5_0': '시설포도_축과병_정상',\n",
       " '6_b5_1': '시설포도_축과병_초기',\n",
       " '6_b5_2': '시설포도_축과병_중기',\n",
       " '6_b5_3': '시설포도_축과병_말기'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@delayed\n",
    "def padding(data, array_len, col_len):\n",
    "    pad = np.zeros((array_len, col_len))\n",
    "    length = min(array_len, len(data))\n",
    "    \n",
    "    pad[:length] = data[:length]\n",
    "    return pad\n",
    "\n",
    "@delayed\n",
    "def img_resize(img):\n",
    "    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32)/255\n",
    "    return img\n",
    "\n",
    "\n",
    "@delayed\n",
    "def label_encoding(label):\n",
    "    global ensemble_label_encoder\n",
    "    encoded_label = ensemble_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_crop(label):\n",
    "    global crop_label_encoder\n",
    "    encoded_label = crop_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_disease(label):\n",
    "    global disease_label_encoder\n",
    "    encoded_label = disease_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_risk(label):\n",
    "    global risk_label_encoder\n",
    "    encoded_label = risk_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "def getlable(jsonpath,type):\n",
    "    if type == \"ensemble\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "        crop = json_file['annotations']['crop']\n",
    "        disease = json_file['annotations']['disease']\n",
    "        risk = json_file['annotations']['risk']\n",
    "        label = f'{crop}_{disease}_{risk}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"crop\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        crop = json_file['annotations']['crop']\n",
    "        label = f'{crop}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"dc\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        disease = json_file['annotations']['disease']\n",
    "        label = f'{disease}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"risk\":  \n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        risk = json_file['annotations']['risk']\n",
    "        label = f'{risk}'\n",
    "        return label\n",
    "\n",
    "    \n",
    "def get_label_list(labelpath_list):\n",
    "    labelarr = np.array([])\n",
    "    labelarr_crop = np.array([])\n",
    "    labelarr_dc = np.array([])\n",
    "    labelarr_risk = np.array([])\n",
    "\n",
    "\n",
    "    # labelarr = da.array([])\n",
    "    for ind,json_path in enumerate(labelpath_list):\n",
    "        # label = label_encoding(getlable(json_path))\n",
    "        # label = da.array(np.array(label_encoding(getlable(json_path))))\n",
    "        label = np.array(label_encoding(getlable(json_path,\"ensemble\")))\n",
    "        labelarr = np.append(labelarr,label)\n",
    "\n",
    "        label_crop = np.array(label_encoding_crop(getlable(json_path,\"crop\")))\n",
    "        labelarr_crop = np.append(labelarr_crop,label_crop)\n",
    "\n",
    "        label_dc = np.array(label_encoding_disease(getlable(json_path,\"dc\")))\n",
    "        labelarr_dc = np.append(labelarr_dc,label_dc)\n",
    "\n",
    "        label_risk = np.array(label_encoding_risk(getlable(json_path,\"risk\")))\n",
    "        labelarr_risk = np.append(labelarr_risk,label_risk)\n",
    "\n",
    "\n",
    "    return labelarr, labelarr_crop, labelarr_dc, labelarr_risk\n",
    "\n",
    "@delayed\n",
    "def imageresize(img):\n",
    "    # img = dask.delayed(cv2.resize)(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    # img = dask.delayed(img.astype(np.float32)/255)  ##픽셀값을 0~1사이로 정규화\n",
    "    # # img = np.transpose(img, (2,0,1))\n",
    "    # return dask.delayed(img.reshape)(-1,224,224,3)\n",
    "    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
    "    # img = np.transpose(img, (2,0,1))\n",
    "    return np.array(img)\n",
    "\n",
    "@delayed\n",
    "def imageresize2(img):\n",
    "    # img = dask.delayed(cv2.resize)(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    # img = dask.delayed(img.astype(np.float32)/255)  ##픽셀값을 0~1사이로 정규화\n",
    "    # # img = np.transpose(img, (2,0,1))\n",
    "    # return dask.delayed(img.reshape)(-1,224,224,3)\n",
    "    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    # img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
    "    # img = np.transpose(img, (2,0,1))\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def get_all_image(imgpath_list,image_size):\n",
    "    imgarr = np.empty((0,image_size,image_size,3), float)\n",
    "    for ind,img_path in enumerate(imgpath_list):\n",
    "        imgarr = delayed(np.append)(imgarr,img, axis = 0)\n",
    "    return imgarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.94 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5767,), (5767,), (5767,), (5767,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "label_list = sorted(glob(os.path.join(path,\"data\",\"train\",\"*\",\"*.json\")))\n",
    "y_train,y_train_crop,y_train_dc,y_train_risk, = get_label_list(label_list)\n",
    "# y_train = y_train.rechunk(5767)\n",
    "results = dask.compute(*y_train)\n",
    "label = np.array(results)\n",
    "# print(label.shape)\n",
    "results_crop = dask.compute(*y_train_crop)\n",
    "label_crop = np.array(results_crop)\n",
    "#\n",
    "results_dc = dask.compute(*y_train_dc)\n",
    "label_disease = np.array(results_dc)\n",
    "#\n",
    "results_risk = dask.compute(*y_train_risk)\n",
    "label_risk = np.array(results_risk)\n",
    "label.shape,label_crop.shape,label_disease.shape,label_risk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block,\n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient,\n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block),\n",
    "                                  transform_sqrt=True,\n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:\n",
    "        features = hog(img, orientations=orient,\n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block),\n",
    "                       transform_sqrt=True,\n",
    "                    #    visualise=vis, feature_vector=feature_vec)\n",
    "                        feature_vector=feature_vec)\n",
    "        return features\n",
    "\n",
    "\n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    color1 = cv2.resize(img[:,:,0], size).ravel()\n",
    "    color2 = cv2.resize(img[:,:,1], size).ravel()\n",
    "    color3 = cv2.resize(img[:,:,2], size).ravel()\n",
    "    return np.hstack((color1, color2, color3))\n",
    "\n",
    "\n",
    "# Define a function to compute color histogram features\n",
    "# NEED TO CHANGE bins_range if reading .png files with mpimg!\n",
    "def color_hist(img, nbins=32):\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "\n",
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9,\n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for ind, image in enumerate(imgs):\n",
    "        \n",
    "        file_features = []\n",
    "\n",
    "        # # png is scale from (0,1)\n",
    "        # image = mpimg.imread(file)\n",
    "\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if color_space != 'RGB':\n",
    "            if color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        else:\n",
    "            feature_image = np.copy(image)\n",
    "\n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "        if hist_feat == True:\n",
    "            # Apply color_hist()\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "            file_features.append(hist_features)\n",
    "            \n",
    "        if hog_feat == True:\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel],\n",
    "                                        orient, pix_per_cell, cell_per_block,\n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)\n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient,\n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            # Append the new feature vector to the features list\n",
    "            file_features.append(hog_features)\n",
    "        features.append(np.concatenate(file_features))\n",
    "    # Return list of feature vectors\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_space = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "# color_space = 'HSV' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9  # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block, which can handel e.g. shadows\n",
    "hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n",
    "# hog_channel = 2 # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (32, 32) # Spatial binning dimensions\n",
    "hist_bins = 32    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def imageresize(img):\n",
    "    # img = dask.delayed(cv2.resize)(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    # img = dask.delayed(img.astype(np.float32)/255)  ##픽셀값을 0~1사이로 정규화\n",
    "    # # img = np.transpose(img, (2,0,1))\n",
    "    # return dask.delayed(img.reshape)(-1,224,224,3)\n",
    "    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
    "    # img = np.transpose(img, (2,0,1))\n",
    "    return np.array(img)\n",
    "#making_img_set\n",
    "row_img = dask_image.imread.imread(os.path.join(path,\"data\",\"train\",\"*\",\"*.jpg\"))\n",
    "train = [imageresize(img) for img in row_img]\n",
    "train_x = np.array(dask.compute(*train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_HOG = extract_features(train_x, color_space=color_space,\n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                        orient=orient, pix_per_cell=pix_per_cell,\n",
    "                        cell_per_block=cell_per_block,\n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat,\n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(train_x_HOG).shape)\n",
    "train_x_HOG_np = np.array(train_x_HOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(train_x_HOG_np, label, test_size=0.2, shuffle= True,stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\xgboost\\core.py:502: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  format(\", \".join(args_msg)), FutureWarning\n",
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 18min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\"tree_method\": \"gpu_hist\", \n",
    "          \"objective\": 'multi:softmax',\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.8,\n",
    "        #   \"single_precision_histogram\" : True,\n",
    "          }\n",
    "\n",
    "start = time.time()\n",
    "XGB_mod1 = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X,y)\n",
    "# print(time.time()-start)\n",
    "y_pred = XGB_mod1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5041303759129423"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139   0   0   8   0   0   0   0   0   1   0  11   0   0   0   0   0   3\n",
      "    0   0   0   0   0   0   0]\n",
      " [  2  16   0   7   0   0   0   0   0   0   0   2   0   0   0   0   0   2\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0  26   4   0   0   0   0   0   0   0   3   0   0   0   0   0   5\n",
      "    0   0   0   0   0   0   0]\n",
      " [  2   0   0 217   0   1   0   1   1   1   1   1   1   0   4   0   2   3\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1   1   0   9   7   6   1   2   0   0   0   2   1   0   1   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1   0   0  13   4   1   0   2   0   0   1   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   5   1   1   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1   0   0   6   0   0   0  21   0   1   4   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1   2   0  10   0   0   0   1   6   0   4   2   0   0   0   0   1   1\n",
      "    0   0   0   0   0   0   0]\n",
      " [  2   1   0   3   1   1   0   1   0  19   1   0   0   0   0   0   0   2\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   7   0   0   0   5   0   0  17   1   0   0   0   0   0   1\n",
      "    0   0   0   0   0   0   0]\n",
      " [  8   1   0   3   0   0   0   0   0   0   0 168   0   0   0   0   0   3\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   2   0   0   0   0   0   0   0   0   0  12   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   0   0  17   0   0   0   2\n",
      "    0   0   0   0   0   0   0]\n",
      " [  1   0   0   5   0   0   0   0   0   0   0   0   0   0  22   0   2   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   2   0   0   0   0   0   0   0   0   0   0   0  30   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0  15   0   0   0   0   0   0   0   1   0   0   4   0  11   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  3   0   0   2   0   0   0   0   0   0   0   7   0   0   0   0   0 154\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   5   0   0   0   0   0   0   0   0   0   0   0   0   0   2\n",
      "    1   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   2\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   6\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   3   0]\n",
      " [  0   0   0   4   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.863     0.858     0.861       162\n",
      "          25      0.762     0.552     0.640        29\n",
      "          28      0.929     0.684     0.788        38\n",
      "          54      0.662     0.923     0.771       235\n",
      "          56      0.538     0.226     0.318        31\n",
      "          57      0.100     0.045     0.063        22\n",
      "          58      0.000     0.000     0.000         8\n",
      "          64      0.618     0.636     0.627        33\n",
      "          68      0.857     0.214     0.343        28\n",
      "          72      0.864     0.613     0.717        31\n",
      "          76      0.607     0.548     0.576        31\n",
      "          79      0.844     0.918     0.880       183\n",
      "         104      0.857     0.857     0.857        14\n",
      "         107      1.000     0.850     0.919        20\n",
      "         118      0.710     0.733     0.721        30\n",
      "         122      1.000     0.938     0.968        32\n",
      "         126      0.688     0.355     0.468        31\n",
      "         129      0.811     0.928     0.865       166\n",
      "         131      1.000     0.125     0.222         8\n",
      "         132      0.000     0.000     0.000         2\n",
      "         135      0.000     0.000     0.000         3\n",
      "         136      0.000     0.000     0.000         6\n",
      "         139      0.000     0.000     0.000         4\n",
      "         141      1.000     1.000     1.000         3\n",
      "         143      0.000     0.000     0.000         4\n",
      "\n",
      "    accuracy                          0.769      1154\n",
      "   macro avg      0.588     0.480     0.504      1154\n",
      "weighted avg      0.752     0.769     0.743      1154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pix_per_cell 키우기 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_space = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "# color_space = 'HSV' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9  # HOG orientations\n",
    "pix_per_cell = 128 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block, which can handel e.g. shadows\n",
    "hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n",
    "# hog_channel = 2 # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (16, 16) # Spatial binning dimensions\n",
    "hist_bins = 32    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_HOG = extract_features(train_x, color_space=color_space,\n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                        orient=orient, pix_per_cell=pix_per_cell,\n",
    "                        cell_per_block=cell_per_block,\n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat,\n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5767, 864)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_x_HOG).shape)\n",
    "train_x_HOG_np = np.array(train_x_HOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(train_x_HOG_np, label, test_size=0.2, shuffle= True,stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\xgboost\\core.py:502: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  format(\", \".join(args_msg)), FutureWarning\n",
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\"tree_method\": \"gpu_hist\", \n",
    "          \"objective\": 'multi:softmax',\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.8,\n",
    "        #   \"single_precision_histogram\" : True,\n",
    "          }\n",
    "\n",
    "start = time.time()\n",
    "XGB_mod1 = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X,y)\n",
    "# print(time.time()-start)\n",
    "y_pred = XGB_mod1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6579989715019952"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139   0   0   6   2   0   0   0   0   0   0   8   0   0   0   0   2   5\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0  17   0   2   0   0   0   0   1   0   0   4   0   0   0   0   0   4\n",
      "    0   0   0   1   0   0   0]\n",
      " [  0   0  30   2   0   0   0   0   0   0   0   4   0   0   0   0   0   2\n",
      "    0   0   0   0   0   0   0]\n",
      " [  3   0   2 222   0   0   0   0   1   0   1   2   0   0   0   0   2   2\n",
      "    0   0   0   0   0   0   0]\n",
      " [  2   0   0   7  12   2   0   3   0   0   1   1   0   0   1   0   1   1\n",
      "    0   0   0   0   0   0   0]\n",
      " [  2   0   0   4   7   4   0   1   0   1   0   2   0   0   0   0   0   1\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   3   2   3   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   7   3   0   0  17   0   0   4   1   0   0   0   0   1   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   8   1   0   0   3  12   0   1   2   0   0   0   0   1   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  2   0   0   8   0   0   0   0   0  15   1   2   1   0   0   0   0   2\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   1   0   4   1   0   0   1   0   0  20   4   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  9   0   0   3   0   0   0   0   0   1   0 165   0   0   0   0   0   5\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   2   4   0   0   0   0   0   0   0   0   6   0   0   0   1   1\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   0   0  19   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   3   0   0   0   0   1   0   0   0   0   0  24   0   2   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  32   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   4   0   0   0   0   0   0   1   0   0   0   1   0  25   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  2   0   0   8   0   0   0   0   0   0   0   7   0   0   0   0   0 149\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   1   2   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    4   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1   1   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   1\n",
      "    0   0   0   4   0   0   0]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2\n",
      "    0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   3   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    2   0   0   0   0   0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.869     0.858     0.863       162\n",
      "          25      0.944     0.586     0.723        29\n",
      "          28      0.857     0.789     0.822        38\n",
      "          54      0.742     0.945     0.831       235\n",
      "          56      0.429     0.387     0.407        31\n",
      "          57      0.444     0.182     0.258        22\n",
      "          58      0.000     0.000     0.000         8\n",
      "          64      0.680     0.515     0.586        33\n",
      "          68      0.800     0.429     0.558        28\n",
      "          72      0.882     0.484     0.625        31\n",
      "          76      0.690     0.645     0.667        31\n",
      "          79      0.813     0.902     0.855       183\n",
      "         104      0.857     0.429     0.571        14\n",
      "         107      1.000     0.950     0.974        20\n",
      "         118      0.923     0.800     0.857        30\n",
      "         122      1.000     1.000     1.000        32\n",
      "         126      0.714     0.806     0.758        31\n",
      "         129      0.837     0.898     0.866       166\n",
      "         131      0.571     0.500     0.533         8\n",
      "         132      1.000     0.500     0.667         2\n",
      "         135      1.000     0.333     0.500         3\n",
      "         136      0.800     0.667     0.727         6\n",
      "         139      1.000     0.250     0.400         4\n",
      "         141      1.000     1.000     1.000         3\n",
      "         143      1.000     0.250     0.400         4\n",
      "\n",
      "    accuracy                          0.800      1154\n",
      "   macro avg      0.794     0.604     0.658      1154\n",
      "weighted avg      0.796     0.800     0.786      1154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv + HOG 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:54224' processes=4 threads=16, memory=31.41 GiB> LocalCluster(b6751d91, 'tcp://127.0.0.1:54224', workers=4, threads=16, memory=31.41 GiB)\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_cuda import LocalCUDACluster\n",
    "#http://localhost:3258/status\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with LocalCluster(dashboard_address=':3258') as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            print(client,cluster)\n",
    "            row_csv = dd.read_csv(os.path.join(path,\"data\",\"train\",\"*\",\"*.csv\"),dtype={'외부 누적일사 평균': 'object','내부 이슬점 최고': 'object',\n",
    "                '내부 이슬점 최저': 'object',\n",
    "                '내부 이슬점 평균': 'object'})\n",
    "            partitions = row_csv.to_delayed()\n",
    "            # datas = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features].values,290,9) for part in partitions]\n",
    "            datas = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features][part != \"-\"].dropna().values,290,9) for part in partitions[:]]\n",
    "            X_train = da.array(dask.compute(*datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(5767,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 114.84 MiB </td>\n",
       "                        <td> 114.84 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (5767, 2610) </td>\n",
       "                        <td> (5767, 2610) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 2 Tasks </td>\n",
       "                        <td> 1 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"104\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"54\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"54\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"54\" y1=\"0\" x2=\"54\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 54.30899947979886,0.0 54.30899947979886,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"27.154500\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >2610</text>\n",
       "  <text x=\"74.308999\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,74.308999,60.000000)\">5767</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<reshape, shape=(5767, 2610), dtype=float64, chunksize=(5767, 2610), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_space = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "# color_space = 'HSV' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9  # HOG orientations\n",
    "pix_per_cell = 128 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block, which can handel e.g. shadows\n",
    "hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n",
    "# hog_channel = 2 # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (16, 16) # Spatial binning dimensions\n",
    "hist_bins = 32    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_HOG = extract_features(train_x, color_space=color_space,\n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                        orient=orient, pix_per_cell=pix_per_cell,\n",
    "                        cell_per_block=cell_per_block,\n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat,\n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_HOG_np = da.array(train_x_HOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 38.01 MiB </td>\n",
       "                        <td> 38.01 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (5767, 864) </td>\n",
       "                        <td> (5767, 864) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 1 Tasks </td>\n",
       "                        <td> 1 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"90\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"40\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"40\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"40\" y1=\"0\" x2=\"40\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 40.700503946930866,0.0 40.700503946930866,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"20.350252\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >864</text>\n",
       "  <text x=\"60.700504\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,60.700504,60.000000)\">5767</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<array, shape=(5767, 864), dtype=float64, chunksize=(5767, 864), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_HOG_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = da.concatenate((train_x_HOG_np,X_train),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 152.85 MiB </td>\n",
       "                        <td> 114.84 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (5767, 3474) </td>\n",
       "                        <td> (5767, 2610) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 5 Tasks </td>\n",
       "                        <td> 2 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"122\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"72\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"72\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"17\" y1=\"0\" x2=\"17\" y2=\"120\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 72.28715103173226,0.0 72.28715103173226,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"36.143576\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3474</text>\n",
       "  <text x=\"92.287151\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,92.287151,60.000000)\">5767</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<concatenate, shape=(5767, 3474), dtype=float64, chunksize=(5767, 2610), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(merged_data, label, test_size=0.2, shuffle= True,stratify=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\xgboost\\core.py:502: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  format(\", \".join(args_msg)), FutureWarning\n",
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\"tree_method\": \"gpu_hist\", \n",
    "          \"objective\": 'multi:softmax',\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.8,\n",
    "        #   \"single_precision_histogram\" : True,\n",
    "          }\n",
    "\n",
    "start = time.time()\n",
    "XGB_mod1 = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X,y)\n",
    "# print(time.time()-start)\n",
    "y_pred = XGB_mod1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9242067288177155"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[162   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0  28   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0  38   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0 235   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  27   3   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  13   8   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   5   1   2   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  33   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  24   0   3   0   0   0   1   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  31   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0  29   0   0   0   0   0   1   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0 182   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  14   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  29   0   1   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  32   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  31   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 166\n",
      "    0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    8   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   2   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   6   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   4   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   3   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    0   0   0   0   0   0   3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000       162\n",
      "          25      1.000     0.966     0.982        29\n",
      "          28      1.000     1.000     1.000        38\n",
      "          54      0.996     1.000     0.998       235\n",
      "          56      0.600     0.871     0.711        31\n",
      "          57      0.667     0.364     0.471        22\n",
      "          58      0.500     0.250     0.333         8\n",
      "          64      1.000     1.000     1.000        33\n",
      "          68      0.960     0.857     0.906        28\n",
      "          72      1.000     1.000     1.000        31\n",
      "          76      0.906     0.935     0.921        31\n",
      "          79      0.995     0.995     0.995       183\n",
      "         104      1.000     1.000     1.000        14\n",
      "         107      1.000     1.000     1.000        20\n",
      "         118      0.967     0.967     0.967        30\n",
      "         122      1.000     1.000     1.000        32\n",
      "         126      0.939     1.000     0.969        31\n",
      "         129      0.994     1.000     0.997       166\n",
      "         131      1.000     1.000     1.000         8\n",
      "         132      1.000     1.000     1.000         2\n",
      "         135      1.000     1.000     1.000         3\n",
      "         136      1.000     1.000     1.000         6\n",
      "         139      1.000     1.000     1.000         4\n",
      "         141      1.000     1.000     1.000         3\n",
      "         143      1.000     0.750     0.857         4\n",
      "\n",
      "    accuracy                          0.971      1154\n",
      "   macro avg      0.941     0.918     0.924      1154\n",
      "weighted avg      0.971     0.971     0.969      1154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test셋 예측후 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\xgboost\\core.py:502: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  format(\", \".join(args_msg)), FutureWarning\n",
      "C:\\Users\\Go\\miniconda3\\envs\\pro1\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\"tree_method\": \"gpu_hist\", \n",
    "          \"objective\": 'multi:softmax',\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.8,\n",
    "        #   \"single_precision_histogram\" : True,\n",
    "          }\n",
    "\n",
    "start = time.time()\n",
    "XGB_G = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(merged_data,label)\n",
    "# print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "Wall time: 46.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"check\")\n",
    "row_csv = dd.read_csv(os.path.join(path,\"data\",\"test\",\"*\",\"*.csv\"),dtype={'외부 누적일사 평균': 'object','내부 이슬점 최고': 'object',\n",
    "    '내부 이슬점 최저': 'object',\n",
    "    '내부 이슬점 평균': 'object'})\n",
    "print(\"check\")\n",
    "partitions = row_csv.to_delayed()\n",
    "print(\"check\")\n",
    "# datas = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features].values,290,9) for part in partitions]\n",
    "datas = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features][part != \"-\"].dropna().values,290,9) for part in partitions]\n",
    "print(\"check\")\n",
    "# X_test = da.array(dask.compute(*datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def imageresize(img):\n",
    "    # img = dask.delayed(cv2.resize)(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    # img = dask.delayed(img.astype(np.float32)/255)  ##픽셀값을 0~1사이로 정규화\n",
    "    # # img = np.transpose(img, (2,0,1))\n",
    "    # return dask.delayed(img.reshape)(-1,224,224,3)\n",
    "    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
    "    # img = np.transpose(img, (2,0,1))\n",
    "    return da.array(img)\n",
    "#making_img_set\n",
    "row_img = dask_image.imread.imread(os.path.join(path,\"data\",\"test\",\"*\",\"*.jpg\"))\n",
    "test = [imageresize(img) for img in row_img]\n",
    "# test_x = da.array(dask.compute(*test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_HOG = extract_features(train_x, color_space=color_space,\n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                        orient=orient, pix_per_cell=pix_per_cell,\n",
    "                        cell_per_block=cell_per_block,\n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat,\n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "Wall time: 24min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# os.path.join(path,\"sample_submission.csv\")\n",
    "submission = pd.read_csv(os.path.join(path,\"sample_submission.csv\"))\n",
    "# submission['label'] = preds\n",
    "minibatch_size = 200\n",
    "data_length = len(submission)\n",
    "loop_num = int(np.floor(data_length/minibatch_size))\n",
    "\n",
    "# for i \n",
    "# xgb.fit(x_prime_train,y_train,xgb_model = xgb)\n",
    "#http://localhost:3258/status\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in range(loop_num):\n",
    "    print(i)\n",
    "    if i !=loop_num-1:\n",
    "        start_p = count*minibatch_size\n",
    "        end_p = (count+1)*minibatch_size\n",
    "        #1\n",
    "        temp = np.array(dask.compute(test[start_p:end_p])[0])\n",
    "        temp_x_feature = extract_features(temp, color_space=color_space,\n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                            orient=orient, pix_per_cell=pix_per_cell,\n",
    "                            cell_per_block=cell_per_block,\n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat,\n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "\n",
    "\n",
    "        #2\n",
    "        temp_x_xgb_feature=da.array(dask.compute(*datas[start_p:end_p])).reshape(len(datas[start_p:end_p]),-1)\n",
    "        temp_x_concated = da.concatenate((temp_x_feature,temp_x_xgb_feature),axis = 1)\n",
    "        #예측\n",
    "        pred_raw = XGB_G.predict(temp_x_concated)\n",
    "        submission.loc[start_p:end_p-1,'label'] = np.array([ensemble_label_decoder[int(val)] for val in pred_raw])\n",
    "        \n",
    "    else:\n",
    "        start_p = count*minibatch_size\n",
    "        #1\n",
    "        #1\n",
    "        temp = np.array(dask.compute(test[start_p:])[0])\n",
    "        temp_x_feature = extract_features(temp, color_space=color_space,\n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                            orient=orient, pix_per_cell=pix_per_cell,\n",
    "                            cell_per_block=cell_per_block,\n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat,\n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "\n",
    "\n",
    "        #2\n",
    "        temp_x_xgb_feature=da.array(dask.compute(*datas[start_p:])).reshape(len(datas[start_p:]),-1)\n",
    "        temp_x_concated = da.concatenate((temp_x_feature,temp_x_xgb_feature),axis = 1)\n",
    "        #예측\n",
    "        pred_raw = XGB_G.predict(temp_x_concated)\n",
    "        submission.loc[start_p:,'label'] = np.array([ensemble_label_decoder[int(val)] for val in pred_raw])\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>6_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>5_b6_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>4_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>3_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>3_b8_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51901</th>\n",
       "      <td>67673</td>\n",
       "      <td>4_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51902</th>\n",
       "      <td>67674</td>\n",
       "      <td>3_b7_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51903</th>\n",
       "      <td>67675</td>\n",
       "      <td>6_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51904</th>\n",
       "      <td>67676</td>\n",
       "      <td>2_a5_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51905</th>\n",
       "      <td>67677</td>\n",
       "      <td>6_00_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51906 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image   label\n",
       "0      10000  6_00_0\n",
       "1      10001  5_b6_1\n",
       "2      10002  4_00_0\n",
       "3      10003  3_00_0\n",
       "4      10004  3_b8_1\n",
       "...      ...     ...\n",
       "51901  67673  4_00_0\n",
       "51902  67674  3_b7_1\n",
       "51903  67675  6_00_0\n",
       "51904  67676  2_a5_2\n",
       "51905  67677  6_00_0\n",
       "\n",
       "[51906 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission.to_csv(os.path.join(path,\"gogo_submission_13.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce43f5afc3da1ea9c2859aca36b65d9af6136ef930ed7edf27ba0e49c79ddf9d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pro1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
