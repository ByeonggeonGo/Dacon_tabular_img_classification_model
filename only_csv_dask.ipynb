{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-98rgb0nf', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-wrmtbyxd', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-rfuxhymv', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-lsyisfe2', purging\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import numpy as np\n",
    "from dask import delayed\n",
    "import dask_image.imread\n",
    "import dask_image.ndfilters\n",
    "import dask_image.ndmeasure\n",
    "import cv2\n",
    "from glob import glob\n",
    "import json\n",
    "import dask.array as da\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "@delayed\n",
    "def padding(data, array_len, col_len):\n",
    "    pad = np.zeros((array_len, col_len))\n",
    "    length = min(array_len, len(data))\n",
    "    pad[:length] = data[:length]\n",
    "    return pad\n",
    "\n",
    "path = os.getcwd()\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "# print(cluster,client)\n",
    "# client.close()\n",
    "\n",
    "# http://localhost:8787/status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making_label_set\n",
    "# \n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "\n",
    "#ensemble_labler\n",
    "label_description = {}\n",
    "for key, value in disease.items():\n",
    "    label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
    "    for disease_code in value:\n",
    "        for risk_code in risk:\n",
    "            label = f'{key}_{disease_code}_{risk_code}'\n",
    "            label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
    "\n",
    "global ensemble_label_encoder\n",
    "ensemble_label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "ensemble_label_decoder = {val:key for key, val in ensemble_label_encoder.items()}\n",
    "\n",
    "#crop_labler\n",
    "crop_label_description = {}\n",
    "for key, value in disease.items():\n",
    "    crop_label_description[f'{key}'] = f'{crop[key]}'\n",
    "    \n",
    "global crop_label_encoder\n",
    "crop_label_encoder = {key:idx for idx, key in enumerate(crop_label_description)}\n",
    "crop_label_decoder = {val:key for key, val in crop_label_encoder.items()}\n",
    "\n",
    "\n",
    "#disease_labler\n",
    "disease_label_description = {}\n",
    "for key, value in disease.items():\n",
    "    disease_label_description[f'00'] = f'{crop[key]}'\n",
    "    for disease_code in value:\n",
    "        label = f'{disease_code}'\n",
    "        disease_label_description[label] = f'{disease_code}'\n",
    "\n",
    "global disease_label_encoder\n",
    "disease_label_encoder = {key:idx for idx, key in enumerate(disease_label_description)}\n",
    "disease_label_decoder = {val:key for key, val in disease_label_encoder.items()}\n",
    "\n",
    "\n",
    "#risk_labler\n",
    "risk_label_description = {}\n",
    "for key, value in risk.items():\n",
    "    label = f'{key}'\n",
    "    risk_label_description[label] = f'{key}'\n",
    "\n",
    "global risk_label_encoder\n",
    "risk_label_encoder = {key:idx for idx, key in enumerate(risk_label_description)}\n",
    "risk_label_decoder = {val:key for key, val in risk_label_encoder.items()}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def label_encoding(label):\n",
    "    global label_encoder\n",
    "    encoded_label = ensemble_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "def getlable(jsonpath):\n",
    "    with open(jsonpath, 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "\n",
    "    crop = json_file['annotations']['crop']\n",
    "    disease = json_file['annotations']['disease']\n",
    "    risk = json_file['annotations']['risk']\n",
    "    label = f'{crop}_{disease}_{risk}'\n",
    "    return label\n",
    "    \n",
    "def get_label_list(labelpath_list):\n",
    "    labelarr = np.array([])\n",
    "    # labelarr = da.array([])\n",
    "    for ind,json_path in enumerate(labelpath_list):\n",
    "        # label = label_encoding(getlable(json_path))\n",
    "        # label = da.array(np.array(label_encoding(getlable(json_path))))\n",
    "        label = np.array(label_encoding(getlable(json_path)))\n",
    "        labelarr = np.append(labelarr,label)\n",
    "    return labelarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_csv = dd.read_csv(\"/mnt/c/data/dacon_parm/train/10027/*.csv\",dtype={'외부 누적일사 평균': 'object','내부 이슬점 최고': 'object',\n",
    "       '내부 이슬점 최저': 'object',\n",
    "       '내부 이슬점 평균': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#making_csv_set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m row_csv \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/c/data/dacon_parm/train/*.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m외부 누적일사 평균\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m내부 이슬점 최고\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m내부 이슬점 최저\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m내부 이슬점 평균\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m partitions \u001b[38;5;241m=\u001b[39m row_csv\u001b[38;5;241m.\u001b[39mto_delayed()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# datas = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features].values,290,9) for part in partitions]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/dask/dataframe/io/csv.py:741\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[0;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    729\u001b[0m     urlpath,\n\u001b[1;32m    730\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    740\u001b[0m ):\n\u001b[0;32m--> 741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43massume_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massume_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_path_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_path_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/dask/dataframe/io/csv.py:520\u001b[0m, in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     paths \u001b[38;5;241m=\u001b[39m get_fs_token_paths(urlpath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)[\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    517\u001b[0m     ]\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# Infer compression from first path\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     compression \u001b[38;5;241m=\u001b[39m infer_compression(\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blocksize \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    523\u001b[0m     blocksize \u001b[38;5;241m=\u001b[39m AUTO_BLOCKSIZE\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#making_csv_set\n",
    "row_csv = dd.read_csv(\"/mnt/c/data/dacon_parm/train/*.csv\",dtype={'외부 누적일사 평균': 'object','내부 이슬점 최고': 'object',\n",
    "       '내부 이슬점 최저': 'object',\n",
    "       '내부 이슬점 평균': 'object'})\n",
    "partitions = row_csv.to_delayed()\n",
    "# datas = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features].values,290,9) for part in partitions]\n",
    "datas = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features][part != \"-\"].dropna().values,290,9) for part in partitions[:]]\n",
    "X_train = np.array(dask.compute(*datas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = sorted(glob(path + \"\\\\data_1\\\\train\\\\*\\\\*.json\"))\n",
    "y_train = get_label_list(label_list)\n",
    "# y_train = y_train.rechunk(5767)\n",
    "results = dask.compute(*y_train)\n",
    "label = np.array(results)\n",
    "# print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5767,), (5767, 290, 9))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhqud\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\core.py:499: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rhqud\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.571685552597046\n",
      "6.348506450653076\n",
      "8.041103601455688\n",
      "9.475648164749146\n",
      "9.165439367294312\n",
      "9.312985181808472\n",
      "9.130616903305054\n",
      "9.028754234313965\n",
      "7.739068508148193\n"
     ]
    }
   ],
   "source": [
    "params = {\"tree_method\": \"gpu_hist\", \n",
    "          \"objective\": 'multi:softmax',\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.8,\n",
    "        #   \"single_precision_histogram\" : True,\n",
    "          }\n",
    "\n",
    "for i in range(len(csv_features)):\n",
    "  start = time.time()\n",
    "  globals()[\"XGB_{}\".format(i)] = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X_train[:,:,i],label)\n",
    "  print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_x(dataset):\n",
    "    global XGB_0, XGB_1, XGB_2, XGB_3, XGB_4, XGB_5, XGB_6, XGB_7, XGB_8 \n",
    "    \n",
    "    x0=XGB_0.predict(dataset[:,:,0])\n",
    "    x1=XGB_1.predict(dataset[:,:,1])\n",
    "    x2=XGB_2.predict(dataset[:,:,2])\n",
    "    x3=XGB_3.predict(dataset[:,:,3])\n",
    "    x4=XGB_4.predict(dataset[:,:,4])\n",
    "    x5=XGB_5.predict(dataset[:,:,5])\n",
    "    x6=XGB_6.predict(dataset[:,:,6])\n",
    "    x7=XGB_7.predict(dataset[:,:,7])\n",
    "    x8=XGB_8.predict(dataset[:,:,8])\n",
    "    arr_x = np.array([x0,x1,x2,x3,x4,x5,x6,x7,x8,]).T\n",
    "    return arr_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=XGB_0.predict(X_train[:,:,0])\n",
    "x1=XGB_1.predict(X_train[:,:,1])\n",
    "x2=XGB_2.predict(X_train[:,:,2])\n",
    "x3=XGB_3.predict(X_train[:,:,3])\n",
    "x4=XGB_4.predict(X_train[:,:,4])\n",
    "x5=XGB_5.predict(X_train[:,:,5])\n",
    "x6=XGB_6.predict(X_train[:,:,6])\n",
    "x7=XGB_7.predict(X_train[:,:,7])\n",
    "x8=XGB_8.predict(X_train[:,:,8])\n",
    "processed_x = np.array([x0,x1,x2,x3,x4,x5,x6,x7,x8,]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhqud\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\core.py:499: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rhqud\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5968475341796875\n"
     ]
    }
   ],
   "source": [
    "params = {\"tree_method\": \"gpu_hist\", \n",
    "          \"objective\": 'multi:softmax',\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.8,\n",
    "        #   \"single_precision_histogram\" : True,\n",
    "          }\n",
    "\n",
    "start = time.time()\n",
    "XGB_G = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(processed_x,label)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_path(label):\n",
    "    # csv_path = f'{file}\\\\{file_name}.csv'\n",
    "    # l_path = os.getcwd() + '\\\\data\\\\test\\\\%s\\\\%s.csv'%{label,label}\n",
    "    csv_path = os.getcwd() +f'\\\\data_1\\\\test\\\\{label}\\\\{label}.csv'\n",
    "    return csv_path\n",
    "    \n",
    "def img_path(label):\n",
    "    # csv_path = f'{file}\\\\{file_name}.csv'\n",
    "    # l_path = os.getcwd() + '\\\\data\\\\test\\\\%s\\\\%s.csv'%{label,label}\n",
    "    img_path = os.getcwd() +f'\\\\data_1\\\\test\\\\{label}\\\\{label}.jpg'\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making_csv_set\n",
    "test_csv = dd.read_csv(path + \"\\\\data_1\\\\test\\\\*\\\\*csv\",dtype={'외부 누적일사 평균': 'object','내부 이슬점 최고': 'object',\n",
    "    '내부 이슬점 최저': 'object',\n",
    "    '내부 이슬점 평균': 'object'})\n",
    "partitions = test_csv.to_delayed()\n",
    "datas_test = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features][part != \"-\"].dropna().values,290,9) for part in partitions[:]]\n",
    "# X_train = np.array(dask.compute(*datas_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array(dask.compute(*datas_test[:1000]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_G.predict(process_x(np.array(dask.compute(*datas_test[:1000]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(path + '\\\\data_1\\\\sample_submission.csv')\n",
    "# submission['label'] = preds\n",
    "minibatch_size = 500\n",
    "data_length = len(submission)\n",
    "loop_num = int(np.floor(data_length/minibatch_size))\n",
    "\n",
    "# for i \n",
    "# xgb.fit(x_prime_train,y_train,xgb_model = xgb)\n",
    "count = 0\n",
    "for i in range(loop_num):\n",
    "# for i in range(2):\n",
    "    print(i)\n",
    "    if i !=loop_num-1:\n",
    "        \n",
    "        start_p = count*minibatch_size\n",
    "        end_p = (count+1)*minibatch_size\n",
    "        \n",
    "        #예측\n",
    "        pred_raw = XGB_G.predict(process_x(np.array(dask.compute(*datas_test[start_p:end_p]))))\n",
    "        submission.loc[start_p:end_p-1,'label'] = np.array([ensemble_label_decoder[int(val)] for val in pred_raw])\n",
    "        \n",
    "    else:\n",
    "        start_p = count*minibatch_size\n",
    "       \n",
    "        #예측\n",
    "        pred_raw = XGB_G.predict(process_x(np.array(dask.compute(*datas_test[start_p:]))))\n",
    "        submission.loc[start_p:,'label'] = np.array([ensemble_label_decoder[int(val)] for val in pred_raw])\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission\\\\gogo_submission_03.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8865460eab3b8858828539624ef2f45d875655b94242e338bcb660acf08eeb38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
