{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/distributed/node.py:161: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 45051 instead\n",
      "  f\"Port {expected} is already in use.\\n\"\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-mzng96qh', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-ikbquyo3', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-w_j_cbax', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-lqao8s2l', purging\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import numpy as np\n",
    "from dask import delayed\n",
    "import dask_image.imread\n",
    "import dask_image.ndfilters\n",
    "import dask_image.ndmeasure\n",
    "import cv2\n",
    "from glob import glob\n",
    "import json\n",
    "import dask.array as da\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "@delayed\n",
    "def padding(data, array_len, col_len):\n",
    "    pad = np.zeros((array_len, col_len))\n",
    "    length = min(array_len, len(data))\n",
    "    pad[:length] = data[:length]\n",
    "    return pad\n",
    "\n",
    "path = os.getcwd()\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "# print(cluster,client)\n",
    "# client.close()\n",
    "\n",
    "# http://localhost:8787/status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making_label_set\n",
    "# \n",
    "# crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "# disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "#            '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "#            '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "#            '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "#            '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "#            '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "# risk = {'1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "\n",
    "# #ensemble_labler\n",
    "# label_description = {}\n",
    "# for key, value in disease.items():\n",
    "#     label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
    "#     for disease_code in value:\n",
    "#         for risk_code in risk:\n",
    "#             label = f'{key}_{disease_code}_{risk_code}'\n",
    "#             label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
    "\n",
    "# global ensemble_label_encoder\n",
    "# ensemble_label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "# ensemble_label_decoder = {val:key for key, val in ensemble_label_encoder.items()}\n",
    "\n",
    "# #crop_labler\n",
    "# crop_label_description = {}\n",
    "# for key, value in disease.items():\n",
    "#     crop_label_description[f'{key}'] = f'{crop[key]}'\n",
    "    \n",
    "# global crop_label_encoder\n",
    "# crop_label_encoder = {key:idx for idx, key in enumerate(crop_label_description)}\n",
    "# crop_label_decoder = {val:key for key, val in crop_label_encoder.items()}\n",
    "\n",
    "\n",
    "# #disease_labler\n",
    "# disease_label_description = {}\n",
    "# for key, value in disease.items():\n",
    "#     disease_label_description[f'00'] = f'{crop[key]}'\n",
    "#     for disease_code in value:\n",
    "#         label = f'{disease_code}'\n",
    "#         disease_label_description[label] = f'{disease_code}'\n",
    "\n",
    "# global disease_label_encoder\n",
    "# disease_label_encoder = {key:idx for idx, key in enumerate(disease_label_description)}\n",
    "# disease_label_decoder = {val:key for key, val in disease_label_encoder.items()}\n",
    "\n",
    "\n",
    "# #risk_labler\n",
    "# risk_label_description = {}\n",
    "# for key, value in risk.items():\n",
    "#     label = f'{key}'\n",
    "#     risk_label_description[label] = f'{key}'\n",
    "\n",
    "# global risk_label_encoder\n",
    "# risk_label_encoder = {key:idx for idx, key in enumerate(risk_label_description)}\n",
    "# risk_label_decoder = {val:key for key, val in risk_label_encoder.items()}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_description = {\n",
    "                    \"1_00_0\" : \"딸기\", \n",
    "                    \"2_00_0\" : \"토마토\",\n",
    "                    \"2_a5_2\" : \"토마토_흰가루병_중기\",\n",
    "                    \"3_00_0\" : \"파프리카\",\n",
    "                    \"3_a9_1\" : \"파프리카_흰가루병_초기\",\n",
    "                    \"3_a9_2\" : \"파프리카_흰가루병_중기\",\n",
    "                    \"3_a9_3\" : \"파프리카_흰가루병_말기\",\n",
    "                    \"3_b3_1\" : \"파프리카_칼슘결핍_초기\",\n",
    "                    \"3_b6_1\" : \"파프리카_다량원소결필(N)_초기\",\n",
    "                    \"3_b7_1\" : \"파프리카_다량원소결필(P)_초기\",\n",
    "                    \"3_b8_1\" : \"파프리카_다량원소결필(K)_초기\",\n",
    "                    \"4_00_0\" : \"오이\",\n",
    "                    \"5_00_0\" : \"고추\",\n",
    "                    \"5_a7_2\" : \"고추_탄저병_중기\",\n",
    "                    \"5_b6_1\" : \"고추_다량원소결필(N)_초기\",\n",
    "                    \"5_b7_1\" : \"고추_다량원소결필(P)_초기\",\n",
    "                    \"5_b8_1\" : \"고추_다량원소결필(K)_초기\",\n",
    "                    \"6_00_0\" : \"시설포도\",\n",
    "                    \"6_a11_1\" : \"시설포도_탄저병_초기\",\n",
    "                    \"6_a11_2\" : \"시설포도_탄저병_중기\",\n",
    "                    \"6_a12_1\" : \"시설포도_노균병_초기\",\n",
    "                    \"6_a12_2\" : \"시설포도_노균병_중기\",\n",
    "                    \"6_b4_1\" : \"시설포도_일소피해_초기\",\n",
    "                    \"6_b4_3\" : \"시설포도_일소피해_말기\",\n",
    "                    \"6_b5_1\" : \"시설포도_축과병_초기\"   }\n",
    "\n",
    "\n",
    "global ensemble_label_encoder\n",
    "ensemble_label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "ensemble_label_decoder = {val:key for key, val in ensemble_label_encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def label_encoding(label):\n",
    "    global label_encoder\n",
    "    encoded_label = ensemble_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "def getlable(jsonpath):\n",
    "    with open(jsonpath, 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "\n",
    "    crop = json_file['annotations']['crop']\n",
    "    disease = json_file['annotations']['disease']\n",
    "    risk = json_file['annotations']['risk']\n",
    "    label = f'{crop}_{disease}_{risk}'\n",
    "    return label\n",
    "    \n",
    "def get_label_list(labelpath_list):\n",
    "    labelarr = np.array([])\n",
    "    # labelarr = da.array([])\n",
    "    for ind,json_path in enumerate(labelpath_list):\n",
    "        # label = label_encoding(getlable(json_path))\n",
    "        # label = da.array(np.array(label_encoding(getlable(json_path))))\n",
    "        label = np.array(label_encoding(getlable(json_path)))\n",
    "        labelarr = np.append(labelarr,label)\n",
    "    return labelarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_csv = dd.read_csv(\"/mnt/c/data/dacon_parm/train/10027/*.csv\",dtype={'외부 누적일사 평균': 'object','내부 이슬점 최고': 'object',\n",
    "       '내부 이슬점 최저': 'object',\n",
    "       '내부 이슬점 평균': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making_csv_set\n",
    "row_csv = dd.read_csv(path+\"/train/*/*.csv\",dtype={'외부 누적일사 평균': 'object','내부 이슬점 최고': 'object',\n",
    "       '내부 이슬점 최저': 'object',\n",
    "       '내부 이슬점 평균': 'object'})\n",
    "partitions = row_csv.to_delayed()\n",
    "# datas = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features].values,290,9) for part in partitions]\n",
    "datas = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features][part != \"-\"].dropna().values,290,9) for part in partitions[:]]\n",
    "X_train = np.array(dask.compute(*datas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = sorted(glob(path+\"/train/*/*.json\"))\n",
    "y_train = get_label_list(label_list)\n",
    "# y_train = y_train.rechunk(5767)\n",
    "results = dask.compute(*y_train)\n",
    "label = np.array(results)\n",
    "# print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5767,), (5767, 290, 9))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"tree_method\": \"gpu_hist\", \n",
    "          \"objective\": 'multi:softmax',\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.8,\n",
    "        #   \"single_precision_histogram\" : True,\n",
    "          }\n",
    "\n",
    "for i in range(len(csv_features)):\n",
    "  start = time.time()\n",
    "  globals()[\"XGB_{}\".format(i)] = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X_train[:,:,i],label)\n",
    "  print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_x(dataset):\n",
    "    global XGB_0, XGB_1, XGB_2, XGB_3, XGB_4, XGB_5, XGB_6, XGB_7, XGB_8 \n",
    "    \n",
    "    x0=XGB_0.predict(dataset[:,:,0])\n",
    "    x1=XGB_1.predict(dataset[:,:,1])\n",
    "    x2=XGB_2.predict(dataset[:,:,2])\n",
    "    x3=XGB_3.predict(dataset[:,:,3])\n",
    "    x4=XGB_4.predict(dataset[:,:,4])\n",
    "    x5=XGB_5.predict(dataset[:,:,5])\n",
    "    x6=XGB_6.predict(dataset[:,:,6])\n",
    "    x7=XGB_7.predict(dataset[:,:,7])\n",
    "    x8=XGB_8.predict(dataset[:,:,8])\n",
    "    arr_x = np.array([x0,x1,x2,x3,x4,x5,x6,x7,x8,]).T\n",
    "    return arr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=XGB_0.predict(X_train[:,:,0])\n",
    "x1=XGB_1.predict(X_train[:,:,1])\n",
    "x2=XGB_2.predict(X_train[:,:,2])\n",
    "x3=XGB_3.predict(X_train[:,:,3])\n",
    "x4=XGB_4.predict(X_train[:,:,4])\n",
    "x5=XGB_5.predict(X_train[:,:,5])\n",
    "x6=XGB_6.predict(X_train[:,:,6])\n",
    "x7=XGB_7.predict(X_train[:,:,7])\n",
    "x8=XGB_8.predict(X_train[:,:,8])\n",
    "processed_x = np.array([x0,x1,x2,x3,x4,x5,x6,x7,x8,]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6878836154937744\n"
     ]
    }
   ],
   "source": [
    "params = {\"tree_method\": \"gpu_hist\", \n",
    "          \"objective\": 'multi:softmax',\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.8,\n",
    "        #   \"single_precision_histogram\" : True,\n",
    "          }\n",
    "\n",
    "start = time.time()\n",
    "XGB_G = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(processed_x,label)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_path(label):\n",
    "    # csv_path = f'{file}\\\\{file_name}.csv'\n",
    "    # l_path = os.getcwd() + '\\\\data\\\\test\\\\%s\\\\%s.csv'%{label,label}\n",
    "    csv_path = os.getcwd() +f'/test/{label}/{label}.csv'\n",
    "    return csv_path\n",
    "    \n",
    "def img_path(label):\n",
    "    # csv_path = f'{file}\\\\{file_name}.csv'\n",
    "    # l_path = os.getcwd() + '\\\\data\\\\test\\\\%s\\\\%s.csv'%{label,label}\n",
    "    img_path = os.getcwd() +f'/test/{label}/{label}.jpg'\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making_csv_set\n",
    "test_csv = dd.read_csv(path+\"/test/*/*.csv\",dtype={'외부 누적일사 평균': 'object','내부 이슬점 최고': 'object',\n",
    "    '내부 이슬점 최저': 'object',\n",
    "    '내부 이슬점 평균': 'object'})\n",
    "partitions = test_csv.to_delayed()\n",
    "datas_test = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features][part != \"-\"].dropna().values,290,9) for part in partitions[:]]\n",
    "# X_train = np.array(dask.compute(*datas_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(path+\"/sample_submission.csv\")\n",
    "# submission['label'] = preds\n",
    "minibatch_size = 500\n",
    "data_length = len(submission)\n",
    "loop_num = int(np.floor(data_length/minibatch_size))\n",
    "\n",
    "# for i \n",
    "# xgb.fit(x_prime_train,y_train,xgb_model = xgb)\n",
    "count = 0\n",
    "for i in range(loop_num):\n",
    "# for i in range(2):\n",
    "    print(i)\n",
    "    if i !=loop_num-1:\n",
    "        \n",
    "        start_p = count*minibatch_size\n",
    "        end_p = (count+1)*minibatch_size\n",
    "        \n",
    "        #예측\n",
    "        pred_raw = XGB_G.predict(process_x(np.array(dask.compute(*datas_test[start_p:end_p]))))\n",
    "        submission.loc[start_p:end_p-1,'label'] = np.array([ensemble_label_decoder[int(val)] for val in pred_raw])\n",
    "        \n",
    "    else:\n",
    "        start_p = count*minibatch_size\n",
    "       \n",
    "        #예측\n",
    "        pred_raw = XGB_G.predict(process_x(np.array(dask.compute(*datas_test[start_p:]))))\n",
    "        submission.loc[start_p:,'label'] = np.array([ensemble_label_decoder[int(val)] for val in pred_raw])\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(path+'/submissions/gogo_submission_06.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8865460eab3b8858828539624ef2f45d875655b94242e338bcb660acf08eeb38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
