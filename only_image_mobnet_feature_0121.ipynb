{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/distributed/node.py:161: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33153 instead\n",
      "  f\"Port {expected} is already in use.\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalCluster(cfded9a8, 'tcp://127.0.0.1:37097', workers=4, threads=16, memory=24.59 GiB) <Client: 'tcp://127.0.0.1:37097' processes=4 threads=16, memory=24.59 GiB>\n",
      "LocalCluster(cfded9a8, 'tcp://127.0.0.1:37097', workers=4, threads=16, memory=24.59 GiB) <Client: 'tcp://127.0.0.1:37097' processes=4 threads=16, memory=24.59 GiB>\n"
     ]
    }
   ],
   "source": [
    "# conda install -c rapidsai -c nvidia -c conda-forge dask-cuda cudatoolkit=11.6\n",
    "# nvidia-smi\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import dask.dataframe as dd\n",
    "import dask.multiprocessing\n",
    "import dask\n",
    "import numpy as np\n",
    "from dask import delayed\n",
    "import dask_image.imread\n",
    "import dask_image.ndfilters\n",
    "import dask_image.ndmeasure\n",
    "import cv2\n",
    "from glob import glob\n",
    "import json\n",
    "import dask.array as da\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "from dask import compute\n",
    "from sklearn.metrics import f1_score\n",
    "# f1_score(y_true, y_pred, average=[‘micro’, ‘macro’, ‘samples’,’weighted’ 중 하나 선택])\n",
    "import tensorflow as tf\n",
    "\n",
    "@delayed\n",
    "def padding(data, array_len, col_len):\n",
    "    pad = np.zeros((array_len, col_len))\n",
    "    length = min(array_len, len(data))\n",
    "    \n",
    "    pad[:length] = data[:length]\n",
    "    return pad\n",
    "\n",
    "@delayed\n",
    "def img_resize(img):\n",
    "    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32)/255\n",
    "    return img\n",
    "\n",
    "path = os.getcwd()\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "#client.close()\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "print(cluster,client)\n",
    "# client.close()\n",
    "# http://localhost:8787/status\n",
    "\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "# from dask.distributed import Client\n",
    "# client.close()\n",
    "# # http://127.0.0.1:34497/status\n",
    "# cluster = LocalCUDACluster()\n",
    "# client = Client(cluster)\n",
    "print(cluster,client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making_label_set\n",
    "\n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'0':'정상','1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "\n",
    "#ensemble_labler\n",
    "label_description = {}\n",
    "for key, value in disease.items():\n",
    "    label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
    "    for disease_code in value:\n",
    "        for risk_code in risk:\n",
    "            label = f'{key}_{disease_code}_{risk_code}'\n",
    "            label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
    "\n",
    "global ensemble_label_encoder\n",
    "ensemble_label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "ensemble_label_decoder = {val:key for key, val in ensemble_label_encoder.items()}\n",
    "\n",
    "#crop_labler\n",
    "crop_label_description = {}\n",
    "for key, value in disease.items():\n",
    "    crop_label_description[f'{key}'] = f'{crop[key]}'\n",
    "    \n",
    "global crop_label_encoder\n",
    "crop_label_encoder = {key:idx for idx, key in enumerate(crop_label_description)}\n",
    "crop_label_decoder = {val:key for key, val in crop_label_encoder.items()}\n",
    "\n",
    "\n",
    "#disease_labler\n",
    "disease_label_description = {}\n",
    "for key, value in disease.items():\n",
    "    disease_label_description[f'00'] = f'{crop[key]}'\n",
    "    for disease_code in value:\n",
    "        label = f'{disease_code}'\n",
    "        disease_label_description[label] = f'{disease_code}'\n",
    "\n",
    "global disease_label_encoder\n",
    "disease_label_encoder = {key:idx for idx, key in enumerate(disease_label_description)}\n",
    "disease_label_decoder = {val:key for key, val in disease_label_encoder.items()}\n",
    "\n",
    "\n",
    "#risk_labler\n",
    "risk_label_description = {}\n",
    "for key, value in risk.items():\n",
    "    label = f'{key}'\n",
    "    risk_label_description[label] = f'{key}'\n",
    "\n",
    "global risk_label_encoder\n",
    "risk_label_encoder = {key:idx for idx, key in enumerate(risk_label_description)}\n",
    "risk_label_decoder = {val:key for key, val in risk_label_encoder.items()}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def label_encoding(label):\n",
    "    global ensemble_label_encoder\n",
    "    encoded_label = ensemble_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_crop(label):\n",
    "    global crop_label_encoder\n",
    "    encoded_label = crop_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_disease(label):\n",
    "    global disease_label_encoder\n",
    "    encoded_label = disease_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_risk(label):\n",
    "    global risk_label_encoder\n",
    "    encoded_label = risk_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "def getlable(jsonpath,type):\n",
    "    if type == \"ensemble\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "        crop = json_file['annotations']['crop']\n",
    "        disease = json_file['annotations']['disease']\n",
    "        risk = json_file['annotations']['risk']\n",
    "        label = f'{crop}_{disease}_{risk}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"crop\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        crop = json_file['annotations']['crop']\n",
    "        label = f'{crop}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"dc\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        disease = json_file['annotations']['disease']\n",
    "        label = f'{disease}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"risk\":  \n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        risk = json_file['annotations']['risk']\n",
    "        label = f'{risk}'\n",
    "        return label\n",
    "\n",
    "    \n",
    "def get_label_list(labelpath_list):\n",
    "    labelarr = np.array([])\n",
    "    labelarr_crop = np.array([])\n",
    "    labelarr_dc = np.array([])\n",
    "    labelarr_risk = np.array([])\n",
    "\n",
    "\n",
    "    # labelarr = da.array([])\n",
    "    for ind,json_path in enumerate(labelpath_list):\n",
    "        # label = label_encoding(getlable(json_path))\n",
    "        # label = da.array(np.array(label_encoding(getlable(json_path))))\n",
    "        label = np.array(label_encoding(getlable(json_path,\"ensemble\")))\n",
    "        labelarr = np.append(labelarr,label)\n",
    "\n",
    "        label_crop = np.array(label_encoding_crop(getlable(json_path,\"crop\")))\n",
    "        labelarr_crop = np.append(labelarr_crop,label_crop)\n",
    "\n",
    "        label_dc = np.array(label_encoding_disease(getlable(json_path,\"dc\")))\n",
    "        labelarr_dc = np.append(labelarr_dc,label_dc)\n",
    "\n",
    "        label_risk = np.array(label_encoding_risk(getlable(json_path,\"risk\")))\n",
    "        labelarr_risk = np.append(labelarr_risk,label_risk)\n",
    "\n",
    "\n",
    "    return labelarr, labelarr_crop, labelarr_dc, labelarr_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def get_all_image(imgpath_list,image_size):\n",
    "    imgarr = np.empty((0,image_size,image_size,3), float)\n",
    "    for ind,img_path in enumerate(imgpath_list):\n",
    "        imgarr = delayed(np.append)(imgarr,img, axis = 0)\n",
    "    return imgarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def imageresize(img):\n",
    "    # img = dask.delayed(cv2.resize)(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    # img = dask.delayed(img.astype(np.float32)/255)  ##픽셀값을 0~1사이로 정규화\n",
    "    # # img = np.transpose(img, (2,0,1))\n",
    "    # return dask.delayed(img.reshape)(-1,224,224,3)\n",
    "    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
    "    # img = np.transpose(img, (2,0,1))\n",
    "    return np.array(img)\n",
    "#making_img_set\n",
    "row_img = dask_image.imread.imread(path+\"/train/*/*.jpg\")\n",
    "train = [imageresize(img) for img in row_img]\n",
    "train_x = np.array(dask.compute(*train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5767,), (5767,), (5767,), (5767,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = sorted(glob(path+\"/train/*/*.json\"))\n",
    "y_train,y_train_crop,y_train_dc,y_train_risk, = get_label_list(label_list)\n",
    "# y_train = y_train.rechunk(5767)\n",
    "results = dask.compute(*y_train)\n",
    "label = np.array(results)\n",
    "# print(label.shape)\n",
    "results_crop = dask.compute(*y_train_crop)\n",
    "label_crop = np.array(results_crop)\n",
    "#\n",
    "results_dc = dask.compute(*y_train_dc)\n",
    "label_dc = np.array(results_dc)\n",
    "#\n",
    "results_risk = dask.compute(*y_train_risk)\n",
    "label_risk = np.array(results_risk)\n",
    "label.shape,label_crop.shape,label_dc.shape,label_risk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모바일넷2 특징추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 09:59:08.891451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-24 09:59:08.898020: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:\n",
      "2022-01-24 09:59:08.898037: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-01-24 09:59:08.898311: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(224,224,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.trainable = False\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  \n",
    "])\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "batch = 100\n",
    "data_length = len(train_x)\n",
    "loop_num = int(np.floor(data_length/batch))\n",
    "\n",
    "train_x_feature = dask.array.zeros((data_length,model(train_x[:1]).numpy().shape[1]))\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in range(loop_num):\n",
    "    print(i)\n",
    "    if i != loop_num-1:\n",
    "        start_p = count*batch\n",
    "        end_p = (count+1)*batch\n",
    "        train_x_feature[start_p:end_p] = model(train_x[start_p:end_p]).numpy()\n",
    "        \n",
    "    else:\n",
    "        start_p = count*batch\n",
    "        end_p = (count+1)*batch\n",
    "        train_x_feature[start_p:] = model(train_x[start_p:]).numpy()\n",
    "            \n",
    "    count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* xgboost 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/xgboost/core.py:502: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  format(\", \".join(args_msg)), FutureWarning\n",
      "/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.09070539474487\n"
     ]
    }
   ],
   "source": [
    "params = {\"tree_method\": \"gpu_hist\", \n",
    "          \"objective\": 'multi:softmax',\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.8,\n",
    "        #   \"single_precision_histogram\" : True,\n",
    "          }\n",
    "\n",
    "start = time.time()\n",
    "XGB = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(train_x_feature,label)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_img_test = dask_image.imread.imread(path+\"/test/*/*.jpg\")\n",
    "test = [imageresize(img) for img in row_img_test]\n",
    "#test_x = np.array(dask.compute(*train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "lask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 10:32:37.368461: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1473970176 exceeds 10% of free system memory.\n",
      "2022-01-24 10:32:37.677623: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1473970176 exceeds 10% of free system memory.\n",
      "2022-01-24 10:32:38.015612: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1473970176 exceeds 10% of free system memory.\n",
      "2022-01-24 10:32:38.320756: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1500408576 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lask test\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(path+\"/sample_submission.csv\")\n",
    "\n",
    "batch = 200\n",
    "data_length = len(test)\n",
    "loop_num = int(np.floor(data_length/batch))\n",
    "\n",
    "count = 0\n",
    "for i in range(loop_num):\n",
    "    print(i)\n",
    "    if i != loop_num-1:\n",
    "        start_p = count*batch\n",
    "        end_p = (count+1)*batch\n",
    "\n",
    "        temp = np.array(compute(test[start_p:end_p])[0])\n",
    "        temp_x_feature = model(temp).numpy()\n",
    "        y_pred = XGB.predict(temp_x_feature)\n",
    "\n",
    "        submission.loc[start_p:end_p-1,'label'] = np.array([ensemble_label_decoder[int(val)] for val in y_pred])\n",
    "        \n",
    "    else:\n",
    "        print(\"lask\")\n",
    "        start_p = count*batch\n",
    "        \n",
    "        temp = np.array(compute(test[start_p:])[0])\n",
    "        temp_x_feature = model(temp).numpy()\n",
    "        y_pred = XGB.predict(temp_x_feature)\n",
    "\n",
    "        submission.loc[start_p:,'label'] = np.array([ensemble_label_decoder[int(val)] for val in y_pred])\n",
    "        print(\"lask test\")\n",
    "            \n",
    "    count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(path+'/submissions/gogo_submission_08.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0977095985b227be1c62f5d3d4b309e6595c46e2b84a98619f98aa2bfdd71ea2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('dacon_parm_01': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
