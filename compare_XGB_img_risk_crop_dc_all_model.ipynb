{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c rapidsai -c nvidia -c conda-forge dask-cuda cudatoolkit=11.6\n",
    "# nvidia-smi\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import dask.dataframe as dd\n",
    "import dask.multiprocessing\n",
    "import dask\n",
    "import numpy as np\n",
    "from dask import delayed\n",
    "import dask_image.imread\n",
    "import dask_image.ndfilters\n",
    "import dask_image.ndmeasure\n",
    "import cv2\n",
    "from glob import glob\n",
    "import json\n",
    "import dask.array as da\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "from dask import compute\n",
    "from sklearn.metrics import f1_score\n",
    "# f1_score(y_true, y_pred, average=[‘micro’, ‘macro’, ‘samples’,’weighted’ 중 하나 선택])\n",
    "import tensorflow as tf\n",
    "\n",
    "@delayed\n",
    "def padding(data, array_len, col_len):\n",
    "    pad = np.zeros((array_len, col_len))\n",
    "    length = min(array_len, len(data))\n",
    "    \n",
    "    pad[:length] = data[:length]\n",
    "    return pad\n",
    "\n",
    "@delayed\n",
    "def img_resize(img):\n",
    "    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32)/255\n",
    "    return img\n",
    "\n",
    "path = os.getcwd()\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# # client.close()\n",
    "# cluster = LocalCluster()\n",
    "# client = Client(cluster)\n",
    "# print(cluster,client)\n",
    "# client.close()\n",
    "# http://localhost:8787/status\n",
    "\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "# from dask.distributed import Client\n",
    "# client.close()\n",
    "# # http://127.0.0.1:34497/status\n",
    "# cluster = LocalCUDACluster()\n",
    "# client = Client(cluster)\n",
    "# print(cluster,client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making_label_set\n",
    "\n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'0':'정상','1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "\n",
    "#ensemble_labler\n",
    "label_description = {}\n",
    "for key, value in disease.items():\n",
    "    label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
    "    for disease_code in value:\n",
    "        for risk_code in risk:\n",
    "            label = f'{key}_{disease_code}_{risk_code}'\n",
    "            label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
    "\n",
    "global ensemble_label_encoder\n",
    "ensemble_label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "ensemble_label_decoder = {val:key for key, val in ensemble_label_encoder.items()}\n",
    "\n",
    "#crop_labler\n",
    "crop_label_description = {}\n",
    "for key, value in disease.items():\n",
    "    crop_label_description[f'{key}'] = f'{crop[key]}'\n",
    "    \n",
    "global crop_label_encoder\n",
    "crop_label_encoder = {key:idx for idx, key in enumerate(crop_label_description)}\n",
    "crop_label_decoder = {val:key for key, val in crop_label_encoder.items()}\n",
    "\n",
    "\n",
    "#disease_labler\n",
    "disease_label_description = {}\n",
    "for key, value in disease.items():\n",
    "    disease_label_description[f'00'] = \"정상\"\n",
    "    for disease_code,value in value.items():\n",
    "        label = f'{disease_code}'\n",
    "        disease_label_description[label] = f'{value}'\n",
    "\n",
    "global disease_label_encoder\n",
    "disease_label_encoder = {key:idx for idx, key in enumerate(disease_label_description)}\n",
    "disease_label_decoder = {val:key for key, val in disease_label_encoder.items()}\n",
    "\n",
    "\n",
    "#risk_labler\n",
    "risk_label_description = {}\n",
    "for key, value in risk.items():\n",
    "    label = f'{key}'\n",
    "    risk_label_description[label] = f'{value}'\n",
    "\n",
    "global risk_label_encoder\n",
    "risk_label_encoder = {key:idx for idx, key in enumerate(risk_label_description)}\n",
    "risk_label_decoder = {val:key for key, val in risk_label_encoder.items()}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': '딸기', '2': '토마토', '3': '파프리카', '4': '오이', '5': '고추', '6': '시설포도'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_label_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': '정상', '1': '초기', '2': '중기', '3': '말기'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_label_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00': '정상',\n",
       " 'a1': '딸기잿빛곰팡이병',\n",
       " 'a2': '딸기흰가루병',\n",
       " 'b1': '냉해피해',\n",
       " 'b6': '다량원소결핍 (N)',\n",
       " 'b7': '다량원소결핍 (P)',\n",
       " 'b8': '다량원소결핍 (K)',\n",
       " 'a5': '토마토흰가루병',\n",
       " 'a6': '토마토잿빛곰팡이병',\n",
       " 'b2': '열과',\n",
       " 'b3': '칼슘결핍',\n",
       " 'a9': '파프리카흰가루병',\n",
       " 'a10': '파프리카잘록병',\n",
       " 'a3': '오이노균병',\n",
       " 'a4': '오이흰가루병',\n",
       " 'a7': '고추탄저병',\n",
       " 'a8': '고추흰가루병',\n",
       " 'a11': '시설포도탄저병',\n",
       " 'a12': '시설포도노균병',\n",
       " 'b4': '일소피해',\n",
       " 'b5': '축과병'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disease_label_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def label_encoding(label):\n",
    "    global ensemble_label_encoder\n",
    "    encoded_label = ensemble_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_crop(label):\n",
    "    global crop_label_encoder\n",
    "    encoded_label = crop_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_disease(label):\n",
    "    global disease_label_encoder\n",
    "    encoded_label = disease_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_risk(label):\n",
    "    global risk_label_encoder\n",
    "    encoded_label = risk_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "def getlable(jsonpath,type):\n",
    "    if type == \"ensemble\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "        crop = json_file['annotations']['crop']\n",
    "        disease = json_file['annotations']['disease']\n",
    "        risk = json_file['annotations']['risk']\n",
    "        label = f'{crop}_{disease}_{risk}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"crop\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        crop = json_file['annotations']['crop']\n",
    "        label = f'{crop}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"dc\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        disease = json_file['annotations']['disease']\n",
    "        label = f'{disease}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"risk\":  \n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        risk = json_file['annotations']['risk']\n",
    "        label = f'{risk}'\n",
    "        return label\n",
    "\n",
    "    \n",
    "def get_label_list(labelpath_list):\n",
    "    labelarr = np.array([])\n",
    "    labelarr_crop = np.array([])\n",
    "    labelarr_dc = np.array([])\n",
    "    labelarr_risk = np.array([])\n",
    "\n",
    "\n",
    "    # labelarr = da.array([])\n",
    "    for ind,json_path in enumerate(labelpath_list):\n",
    "        # label = label_encoding(getlable(json_path))\n",
    "        # label = da.array(np.array(label_encoding(getlable(json_path))))\n",
    "        label = np.array(label_encoding(getlable(json_path,\"ensemble\")))\n",
    "        labelarr = np.append(labelarr,label)\n",
    "\n",
    "        label_crop = np.array(label_encoding_crop(getlable(json_path,\"crop\")))\n",
    "        labelarr_crop = np.append(labelarr_crop,label_crop)\n",
    "\n",
    "        label_dc = np.array(label_encoding_disease(getlable(json_path,\"dc\")))\n",
    "        labelarr_dc = np.append(labelarr_dc,label_dc)\n",
    "\n",
    "        label_risk = np.array(label_encoding_risk(getlable(json_path,\"risk\")))\n",
    "        labelarr_risk = np.append(labelarr_risk,label_risk)\n",
    "\n",
    "\n",
    "    return labelarr, labelarr_crop, labelarr_dc, labelarr_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def get_all_image(imgpath_list,image_size):\n",
    "    imgarr = np.empty((0,image_size,image_size,3), float)\n",
    "    for ind,img_path in enumerate(imgpath_list):\n",
    "        imgarr = delayed(np.append)(imgarr,img, axis = 0)\n",
    "    return imgarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "@delayed\n",
    "def imageresize(img):\n",
    "    # img = dask.delayed(cv2.resize)(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    # img = dask.delayed(img.astype(np.float32)/255)  ##픽셀값을 0~1사이로 정규화\n",
    "    # # img = np.transpose(img, (2,0,1))\n",
    "    # return dask.delayed(img.reshape)(-1,224,224,3)\n",
    "    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
    "    # img = np.transpose(img, (2,0,1))\n",
    "    return np.array(img)\n",
    "#making_img_set\n",
    "row_img = dask_image.imread.imread(os.path.join(path,\"data\",\"train\",\"*\",\"*.jpg\"))\n",
    "train = [imageresize(img) for img in row_img]\n",
    "train_x = np.array(dask.compute(*train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5767,), (5767,), (5767,), (5767,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "label_list = sorted(glob(os.path.join(path,\"data\",\"train\",\"*\",\"*.json\")))\n",
    "y_train,y_train_crop,y_train_dc,y_train_risk, = get_label_list(label_list)\n",
    "# y_train = y_train.rechunk(5767)\n",
    "results = dask.compute(*y_train)\n",
    "label = np.array(results)\n",
    "# print(label.shape)\n",
    "results_crop = dask.compute(*y_train_crop)\n",
    "label_crop = np.array(results_crop)\n",
    "#\n",
    "results_dc = dask.compute(*y_train_dc)\n",
    "label_dc = np.array(results_dc)\n",
    "#\n",
    "results_risk = dask.compute(*y_train_risk)\n",
    "label_risk = np.array(results_risk)\n",
    "label.shape,label_crop.shape,label_dc.shape,label_risk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(224,224,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.trainable = False\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  \n",
    "])\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch = 100\n",
    "data_length = len(train_x)\n",
    "loop_num = int(np.floor(data_length/batch))\n",
    "\n",
    "train_x_feature = dask.array.zeros((data_length,model(train_x[:1]).numpy().shape[1]))\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in range(loop_num):\n",
    "    print(i)\n",
    "    if i != loop_num-1:\n",
    "        start_p = count*batch\n",
    "        end_p = (count+1)*batch\n",
    "        train_x_feature[start_p:end_p] = model(train_x[start_p:end_p]).numpy()\n",
    "        \n",
    "    else:\n",
    "        start_p = count*batch\n",
    "        end_p = (count+1)*batch\n",
    "        train_x_feature[start_p:] = model(train_x[start_p:]).numpy()\n",
    "            \n",
    "    count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crop모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 조정\n",
    "y_all = label_crop\n",
    "cnn_out_dim = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(train_x_feature, y_all, test_size=0.2, shuffle= True,stratify=y_all)\n",
    "X = da.array(X).rechunk((1000,cnn_out_dim))\n",
    "X_test = da.array(X_test).rechunk((1000,cnn_out_dim))\n",
    "y = da.array(y).rechunk((1000,1))\n",
    "y_test = da.array(y_test).rechunk((1000,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dask' has no attribute 'distributed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1568/1722474025.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mcluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLocalCluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# X and y must be Dask dataframes or arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'dask' has no attribute 'distributed'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cluster = dask.distributed.LocalCluster()\n",
    "    client = dask.distributed.Client(cluster)\n",
    "\n",
    "    # X and y must be Dask dataframes or arrays\n",
    "    \n",
    "\n",
    "    dtrain = xgb.dask.DaskDMatrix(client, X, y)\n",
    "\n",
    "    params = {\"verbosity\": 2, \"tree_method\": \"hist\", \"objective\": 'multi:softmax',}\n",
    "    output = xgb.dask.train(\n",
    "        client,\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=4,\n",
    "        evals=[(dtrain, \"train\")],\n",
    "    )\n",
    "    y_pred = xgb.dask.predict(client, output, X_test).compute()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce43f5afc3da1ea9c2859aca36b65d9af6136ef930ed7edf27ba0e49c79ddf9d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pro1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
