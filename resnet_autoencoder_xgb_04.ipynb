{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50 + autoencoder로 각각 이미지와 시계열 셋을 전처리한 후 XGB로 파인튠하여 최종분류\n",
    "*실제데이터로 트라이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*해결과제\n",
    "1. 10번쨰열 결측치 처리\n",
    "2. 데이터 중복시간있는것 처리 -> 10번째열 있는 자료만 쓰려고 하였으나 10번쨰열이 없는 데이터도 상당히 존재함\n",
    "3. 1~9 -> 10 하는 간단한 모델 적합 후 결측치 처리하는 방향으로 고려중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json \n",
    "import random\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 sample data는 파프리카와 시설포도 2종류의 작물만 존재\n",
    "label_description = {\n",
    " '3_00_0': '파프리카_정상',\n",
    " '3_a9_1': '파프리카흰가루병_초기',\n",
    " '3_a9_2': '파프리카흰가루병_중기',\n",
    " '3_a9_3': '파프리카흰가루병_말기',\n",
    " '3_a10_1': '파프리카잘록병_초기',\n",
    " '3_a10_2': '파프리카잘록병_중기',\n",
    " '3_a10_3': '파프리카잘록병_말기',\n",
    " '3_b3_1': '칼슘결핍_초기',\n",
    " '3_b3_2': '칼슘결핍_중기',\n",
    " '3_b3_3': '칼슘결핍_말기',\n",
    " '3_b6_1': '다량원소결핍 (N)_초기',\n",
    " '3_b6_2': '다량원소결핍 (N)_중기',\n",
    " '3_b6_3': '다량원소결핍 (N)_말기',\n",
    " '3_b7_1': '다량원소결핍 (P)_초기',\n",
    " '3_b7_2': '다량원소결핍 (P)_중기',\n",
    " '3_b7_3': '다량원소결핍 (P)_말기',\n",
    " '3_b8_1': '다량원소결핍 (K)_초기',\n",
    " '3_b8_2': '다량원소결핍 (K)_중기',\n",
    " '3_b8_3': '다량원소결핍 (K)_말기',\n",
    " '6_00_0': '시설포도_정상',\n",
    " '6_a11_1': '시설포도탄저병_초기',\n",
    " '6_a11_2': '시설포도탄저병_중기',\n",
    " '6_a11_3': '시설포도탄저병_말기',\n",
    " '6_a12_1': '시설포도노균병_초기',\n",
    " '6_a12_2': '시설포도노균병_중기',\n",
    " '6_a12_3': '시설포도노균병_말기',\n",
    " '6_b4_1': '일소피해_초기',\n",
    " '6_b4_2': '일소피해_중기',\n",
    " '6_b4_3': '일소피해_말기',\n",
    " '6_b5_1': '축과병_초기',\n",
    " '6_b5_2': '축과병_중기',\n",
    " '6_b5_3': '축과병_말기',\n",
    "}\n",
    "\n",
    "label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "label_decoder = {val:key for key, val in label_encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5759/5759 [00:57<00:00, 100.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'내부 온도 1 평균': [3.4, 47.3],\n",
       " '내부 온도 1 최고': [3.4, 47.6],\n",
       " '내부 온도 1 최저': [3.3, 47.0],\n",
       " '내부 습도 1 평균': [23.7, 100.0],\n",
       " '내부 습도 1 최고': [25.9, 100.0],\n",
       " '내부 습도 1 최저': [0.0, 100.0],\n",
       " '내부 이슬점 평균': [0.1, 34.5],\n",
       " '내부 이슬점 최고': [0.2, 34.7],\n",
       " '내부 이슬점 최저': [0.0, 34.4]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분석에 사용할 feature 선택\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "csv_files = sorted(glob(path + '\\\\data\\\\train\\\\*\\\\*.csv'))\n",
    "\n",
    "temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
    "max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "\n",
    "# feature 별 최대값, 최솟값 계산\n",
    "for csv in tqdm(csv_files[1:]):\n",
    "    temp_csv = pd.read_csv(csv)[csv_features]\n",
    "    temp_csv = temp_csv.replace('-',np.nan).dropna()#.astype(float)\n",
    "    if len(temp_csv) == 0:\n",
    "        continue\n",
    "    temp_csv = temp_csv.astype(float)\n",
    "    temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "    max_arr = np.max([max_arr,temp_max], axis=0)\n",
    "    min_arr = np.min([min_arr,temp_min], axis=0)\n",
    "\n",
    "# feature 별 최대값, 최솟값 dictionary 생성\n",
    "csv_feature_dict = {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
    "csv_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 설명 csv 파일 참조\n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "label_description = {}\n",
    "for key, value in disease.items():\n",
    "    label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
    "    for disease_code in value:\n",
    "        for risk_code in risk:\n",
    "            label = f'{key}_{disease_code}_{risk_code}'\n",
    "            label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
    "\n",
    "label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "label_decoder = {val:key for key, val in label_encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, files, labels=None, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.files = files\n",
    "        self.csv_feature_dict = csv_feature_dict\n",
    "        self.csv_feature_check = [0]*len(self.files)\n",
    "        self.csv_features = [None]*len(self.files)\n",
    "        self.max_len = 24 * 6\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        file = self.files[i]\n",
    "        file_name = file.split('\\\\')[-1]\n",
    "        \n",
    "        # csv\n",
    "        if self.csv_feature_check[i] == 0:\n",
    "            csv_path = f'{file}\\\\{file_name}.csv'\n",
    "            # csv_path = f'{file_name}.csv'\n",
    "            df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
    "            df = df.replace('-', 0)\n",
    "            # MinMax scaling\n",
    "            for col in df.columns:\n",
    "                df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
    "                df[col] = df[col] / (self.csv_feature_dict[col][1]-self.csv_feature_dict[col][0])\n",
    "            # zero padding\n",
    "            pad = np.zeros((self.max_len, len(df.columns)))\n",
    "            length = min(self.max_len, len(df))\n",
    "            pad[-length:] = df.to_numpy()[-length:]\n",
    "            # transpose to sequential data\n",
    "            csv_feature = pad\n",
    "            self.csv_features[i] = csv_feature\n",
    "            self.csv_feature_check[i] = 1\n",
    "        else:\n",
    "            csv_feature = self.csv_features[i]\n",
    "        \n",
    "        # image\n",
    "        image_path = f'{file}\\\\{file_name}.jpg'\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "        img = img.astype(np.float32)/255\n",
    "        # img = np.transpose(img, (2,0,1))\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            json_path = f'{file}\\\\{file_name}.json'\n",
    "            with open(json_path, 'r') as f:\n",
    "                json_file = json.load(f)\n",
    "            \n",
    "            crop = json_file['annotations']['crop']\n",
    "            disease = json_file['annotations']['disease']\n",
    "            risk = json_file['annotations']['risk']\n",
    "            label = f'{crop}_{disease}_{risk}'\n",
    "            \n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'csv_feature' : torch.tensor(csv_feature, dtype=torch.float32),\n",
    "                'label' : torch.tensor(self.label_encoder[label], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'csv_feature' : torch.tensor(csv_feature, dtype=torch.float32)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 256\n",
    "# class_n = len(label_encoder)\n",
    "# learning_rate = 1e-4\n",
    "# embedding_dim = 512\n",
    "\n",
    "num_features = len(csv_feature_dict)\n",
    "max_len = 144\n",
    "# dropout_rate = 0.1\n",
    "# epochs = 10\n",
    "# vision_pretrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sorted(glob(path + '\\\\data\\\\train\\\\*'))[:100]\n",
    "test = sorted(glob(path + '\\\\data\\\\test\\\\*'))\n",
    "\n",
    "try:\n",
    "    train.remove('d:\\\\OneDrive - UOS\\\\allrepos\\\\dacon_diagnosis\\\\data\\\\train\\\\20643')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "labelsss = pd.read_csv('data\\\\train.csv')['label']\n",
    "# train, val = train_test_split(train, test_size=0.2, stratify=labelsss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train)\n",
    "# val_dataset = CustomDataset(val)\n",
    "test_dataset = CustomDataset(test, mode = 'test')\n",
    "\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=16, shuffle=True)\n",
    "# val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=16, shuffle=True)\n",
    "# # val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset ,shuffle=False)\n",
    "# val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvarr = np.empty((0,144,9), float)\n",
    "# imgarr = np.empty((0,224,224,3), float)\n",
    "# lablearr = np.array([])\n",
    "                 \n",
    "# for batch, batch_item in enumerate(train_dataloader):\n",
    "#     # if batch == 1:\n",
    "#     #     pass\n",
    "#     # else:\n",
    "#     #     break\n",
    "#     csvarr = np.append(csvarr,batch_item['csv_feature'], axis = 0)\n",
    "#     imgarr = np.append(imgarr,batch_item['img'], axis = 0)\n",
    "#     lablearr = np.append(lablearr,batch_item['label'])\n",
    "        \n",
    "       \n",
    "                \n",
    "# csvarr_test = np.empty((0,144,9), float)\n",
    "# imgarr_test = np.empty((0,224,224,3), float)\n",
    "# # lablearr_test = np.array([])\n",
    "                 \n",
    "# for batch, batch_item in enumerate(test_dataloader):\n",
    "#     # if batch == 1:\n",
    "#     #     pass\n",
    "#     # else:\n",
    "#     #     break\n",
    "#     csvarr_test = np.append(csvarr_test,batch_item['csv_feature'], axis = 0)\n",
    "#     imgarr_test = np.append(imgarr_test,batch_item['img'], axis = 0)\n",
    "#     # lablearr_test = np.append(lablearr_test,batch_item['label'])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'd:\\\\OneDrive - UOS\\\\allrepos\\\\dacon_diagnosis\\\\data\\\\train\\\\66997\\\\66997.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2008/2449813095.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_item\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;31m# if batch == 1:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m#     pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2008/1207039917.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mcsv_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{file}\\\\{file_name}.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m# csv_path = f'{file_name}.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_feature_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m# MinMax scaling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'd:\\\\OneDrive - UOS\\\\allrepos\\\\dacon_diagnosis\\\\data\\\\train\\\\66997\\\\66997.csv'"
     ]
    }
   ],
   "source": [
    "csvarr = np.empty((0,144,9), float)\n",
    "imgarr = np.empty((0,224,224,3), float)\n",
    "lablearr = np.array([])\n",
    "\n",
    "unit = 500\n",
    "\n",
    "for i in range(12):\n",
    "    i = i+1\n",
    "    train = sorted(glob(path + '\\\\data\\\\train\\\\*'))[(i-1)*unit:i*unit]\n",
    "    test = sorted(glob(path + '\\\\data\\\\test\\\\*'))\n",
    "\n",
    "    try:\n",
    "        train.remove('d:\\\\OneDrive - UOS\\\\allrepos\\\\dacon_diagnosis\\\\data\\\\train\\\\20643')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    labelsss = pd.read_csv('data\\\\train.csv')['label']\n",
    "    # train, val = train_test_split(train, test_size=0.2, stratify=labelsss)\n",
    "    train_dataset = CustomDataset(train)\n",
    "    # val_dataset = CustomDataset(val)\n",
    "    test_dataset = CustomDataset(test, mode = 'test')\n",
    "\n",
    "    # train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=16, shuffle=True)\n",
    "    # val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\n",
    "    # test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\n",
    "    # train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=16, shuffle=True)\n",
    "    # # val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\n",
    "    # test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset ,shuffle=False)\n",
    "    # val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,shuffle=False)\n",
    "    \n",
    "\n",
    "                 \n",
    "    for batch, batch_item in enumerate(train_dataloader):\n",
    "        # if batch == 1:\n",
    "        #     pass\n",
    "        # else:\n",
    "        #     break\n",
    "        csvarr = np.append(csvarr,batch_item['csv_feature'], axis = 0)\n",
    "        imgarr = np.append(imgarr,batch_item['img'], axis = 0)\n",
    "        lablearr = np.append(lablearr,batch_item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvarr_test = np.empty((0,144,9), float)\n",
    "imgarr_test = np.empty((0,224,224,3), float)\n",
    "# lablearr_test = np.array([])\n",
    "                 \n",
    "for batch, batch_item in enumerate(test_dataloader):\n",
    "    # if batch == 1:\n",
    "    #     pass\n",
    "    # else:\n",
    "    #     break\n",
    "    csvarr_test = np.append(csvarr_test,batch_item['csv_feature'], axis = 0)\n",
    "    imgarr_test = np.append(imgarr_test,batch_item['img'], axis = 0)\n",
    "    # lablearr_test = np.append(lablearr_test,batch_item['label'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [csvarr,imgarr]\n",
    "y_train = lablearr\n",
    "\n",
    "x_test = [csvarr_test,imgarr_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Eencoder(keras.layers.Layer):\n",
    "    global max_len\n",
    "    def __init__(self):\n",
    "        super(Eencoder, self).__init__()\n",
    "        self.block1_layer1 = tf.keras.layers.Conv1D(9, 24*2, activation='relu',)#input_shape=input_shape[1:])\n",
    "        self.block1_layer2 = tf.keras.layers.Conv1D(18, 24*2+1, activation='relu',)#input_shape=input_shape[1:])\n",
    "        self.block1_layer3 = tf.keras.layers.Conv1D(36, 24*2+1, activation='relu',)#input_shape=input_shape[1:])\n",
    "         \n",
    "    def call(self, inputs):\n",
    "        #LSTM파트\n",
    "        lstm_x = self.block1_layer1(inputs)\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        lstm_x = self.block1_layer2(lstm_x)\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        lstm_x = self.block1_layer3(lstm_x)\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        \n",
    "        return lstm_x\n",
    "class Ddecoder(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Ddecoder, self).__init__()\n",
    "        self.block1_layer1 = tf.keras.layers.Conv1DTranspose(3,  24*2, activation='relu',)#input_shape=input_shape[1:])\n",
    "        self.block1_layer2 = tf.keras.layers.Conv1DTranspose(6,  24*2+1, activation='relu',)#input_shape=input_shape[1:])\n",
    "        self.block1_layer3 = tf.keras.layers.Conv1DTranspose(9,  24*2+1, activation='relu',)#input_shape=input_shape[1:])\n",
    "        \n",
    "         \n",
    "    def call(self, inputs):\n",
    "        #LSTM파트\n",
    "        x = self.block1_layer1(inputs)\n",
    "        \n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.block1_layer2(x)\n",
    "        \n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.block1_layer3(x)\n",
    "       \n",
    "        return x\n",
    "\n",
    "# Eencoder()(x_train[0][:2,:,:]).shape\n",
    "# Ddecoder()(Eencoder()(x_train[0][:2,:,:]))\n",
    "\n",
    "class Autoencoder(tf.keras.Model): \n",
    "  def __init__(self,): \n",
    "    super(Autoencoder, self).__init__() \n",
    "    self.encoder = Eencoder() \n",
    "    self.decoder = Ddecoder() \n",
    "  \n",
    "  def call(self, input): \n",
    "    code = self.encoder(input) \n",
    "    reconstructed = self.decoder(code) \n",
    "    return reconstructed\n",
    "\n",
    "def loss(model, original): \n",
    "  reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(original), original))) \n",
    "  return reconstruction_error\n",
    "\n",
    "def train(loss, model, opt, original): \n",
    "  with tf.GradientTape() as tape: \n",
    "    gradients = tape.gradient(loss(model, original), model.trainable_variables) \n",
    "    gradient_variables = zip(gradients, model.trainable_variables) \n",
    "    opt.apply_gradients(gradient_variables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 0.3380\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3310\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3147\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2815\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2307\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2131\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2191\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1809\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1672\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1648\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1570\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1429\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1313\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1330\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1318\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1201\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1142\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1142\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1119\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1054\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0992\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0982\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0973\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0915\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0869\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0857\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0839\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0798\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0761\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0750\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0734\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0698\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0672\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0662\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0643\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0616\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0601\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0592\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0573\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0555\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0547\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0537\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0523\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0513\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0507\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0497\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0486\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0479\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0471\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29d88164d00>"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automodel = Autoencoder()\n",
    "opt = tf.optimizers.Adam()\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "automodel.compile(optimizer=opt, loss=loss_fn)\n",
    "automodel.fit(x_train[0], x_train[0], \n",
    "                 batch_size=1000, \n",
    "                 epochs=50,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testset에 오토인코더 손실함수 테스트:  tf.Tensor(0.04912294, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_pred = automodel(x_test[0])\n",
    "print(\"testset에 오토인코더 손실함수 테스트: \",loss_fn(x_test[0],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1, 36)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automodel.encoder(x_train[0]).numpy().shape#.reshape(-1,36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1000) (100, 36)\n",
      "(100, 1000) (100, 36)\n"
     ]
    }
   ],
   "source": [
    "class preprmodel(keras.Model):\n",
    "    def __init__(self, imgmodel,autoencoder,name = None):\n",
    "        super(preprmodel, self).__init__()\n",
    "        self.imgmodel = imgmodel\n",
    "        self.autoencoder = autoencoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x1 = self.imgmodel(inputs[1])\n",
    "        x2 = self.autoencoder.encoder(inputs[0]).numpy().reshape(-1,36)\n",
    "\n",
    "        print(x1.shape,x2.shape)\n",
    "        x = tf.concat([x1,x2],axis=1)\n",
    "        \n",
    "        return(x)\n",
    "model_RESNET50 = ResNet50(weights='imagenet')\n",
    "prepromodel = preprmodel(model_RESNET50,automodel)\n",
    "x_prime_train = prepromodel(x_train)\n",
    "x_prime_test = prepromodel(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(colsample_bytree=0.7, objective='multi:softprob',\n",
       "              random_state=100, scale_pos_weight=0.2, subsample=0.7)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(random_state=100, subsample= 0.7, colsample_bytree=0.7, scale_pos_weight=0.2)\n",
    "xgb.fit(x_prime_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>preds</th>\n",
       "      <th>시설포도_정상</th>\n",
       "      <th>시설포도노균병_중기</th>\n",
       "      <th>시설포도탄저병_초기</th>\n",
       "      <th>일소피해_말기</th>\n",
       "      <th>일소피해_초기</th>\n",
       "      <th>파프리카_정상</th>\n",
       "      <th>파프리카흰가루병_말기</th>\n",
       "      <th>파프리카흰가루병_중기</th>\n",
       "      <th>파프리카흰가루병_초기</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>시설포도_정상</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>시설포도노균병_중기</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>시설포도노균병_초기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>시설포도탄저병_초기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>일소피해_말기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>일소피해_초기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>파프리카_정상</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>파프리카잘록병_말기</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>파프리카흰가루병_말기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>파프리카흰가루병_중기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>파프리카흰가루병_초기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "preds        시설포도_정상  시설포도노균병_중기  시설포도탄저병_초기  일소피해_말기  일소피해_초기  파프리카_정상  \\\n",
       "answer                                                                    \n",
       "시설포도_정상           39           0           0        0        0        0   \n",
       "시설포도노균병_중기         0           3           0        0        0        0   \n",
       "시설포도노균병_초기         0           0           0        0        0        0   \n",
       "시설포도탄저병_초기         0           0           2        0        0        0   \n",
       "일소피해_말기            0           0           0        1        0        0   \n",
       "일소피해_초기            0           0           0        0        1        0   \n",
       "파프리카_정상            0           0           0        0        0       34   \n",
       "파프리카잘록병_말기         1           0           0        0        0        0   \n",
       "파프리카흰가루병_말기        0           0           0        0        0        0   \n",
       "파프리카흰가루병_중기        0           0           0        0        0        0   \n",
       "파프리카흰가루병_초기        0           0           0        0        0        0   \n",
       "\n",
       "preds        파프리카흰가루병_말기  파프리카흰가루병_중기  파프리카흰가루병_초기  \n",
       "answer                                              \n",
       "시설포도_정상                0            0            0  \n",
       "시설포도노균병_중기             0            0            0  \n",
       "시설포도노균병_초기             0            1            0  \n",
       "시설포도탄저병_초기             0            0            0  \n",
       "일소피해_말기                0            0            0  \n",
       "일소피해_초기                0            0            0  \n",
       "파프리카_정상                0            0            0  \n",
       "파프리카잘록병_말기             0            0            0  \n",
       "파프리카흰가루병_말기            1            0            1  \n",
       "파프리카흰가루병_중기            0            0            7  \n",
       "파프리카흰가루병_초기            0            3            6  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=xgb.predict(x_prime_test)\n",
    "# answer = np.array([label_description[label_decoder[int(val)]] for val in y_test])\n",
    "# predss = np.array([label_description[label_decoder[int(val)]] for val in y_pred])\n",
    "\n",
    "# new_crosstab = pd.crosstab(answer, predss, rownames=['answer'], colnames=['preds'])\n",
    "# new_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  6  3  0  0  0  0  0  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 39  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  2  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      1.000     1.000     1.000        34\n",
      "         1.0      0.429     0.667     0.522         9\n",
      "         2.0      0.000     0.000     0.000         7\n",
      "         3.0      1.000     0.500     0.667         2\n",
      "         6.0      0.000     0.000     0.000         1\n",
      "        19.0      0.975     1.000     0.987        39\n",
      "        20.0      1.000     1.000     1.000         2\n",
      "        23.0      0.000     0.000     0.000         1\n",
      "        24.0      1.000     1.000     1.000         3\n",
      "        26.0      1.000     1.000     1.000         1\n",
      "        28.0      1.000     1.000     1.000         1\n",
      "\n",
      "    accuracy                          0.870       100\n",
      "   macro avg      0.673     0.652     0.652       100\n",
      "weighted avg      0.849     0.870     0.855       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhqud\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rhqud\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rhqud\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8865460eab3b8858828539624ef2f45d875655b94242e338bcb660acf08eeb38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
