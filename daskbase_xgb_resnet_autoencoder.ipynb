{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-image --upgrade-strategy only-if-needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import dask_image.imread\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json \n",
    "import random\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed, compute\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalCluster(28f79ad0, 'tcp://127.0.0.1:60417', workers=4, threads=16, memory=31.93 GiB) <Client: 'tcp://127.0.0.1:60417' processes=4 threads=16, memory=31.93 GiB>\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "print(cluster,client)\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "xgboost.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 sample data는 파프리카와 시설포도 2종류의 작물만 존재\n",
    "# 변수 설명 csv 파일 참조\n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "label_description = {}\n",
    "for key, value in disease.items():\n",
    "    label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
    "    for disease_code in value:\n",
    "        for risk_code in risk:\n",
    "            label = f'{key}_{disease_code}_{risk_code}'\n",
    "            label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
    "\n",
    "global label_encoder\n",
    "label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "label_decoder = {val:key for key, val in label_encoder.items()}\n",
    "\n",
    "\n",
    "\n",
    "@delayed\n",
    "def label_encoding(label):\n",
    "    global label_encoder\n",
    "    encoded_label = label_encoder[label]\n",
    "    return encoded_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller():\n",
    "    def __init__(self,csv_feature_dict,len_of_df,feat_len_of_df):\n",
    "        \n",
    "        self.csv_feature_dict = csv_feature_dict\n",
    "        self.len_of_df = len_of_df\n",
    "        self.feat_len_of_df = feat_len_of_df\n",
    "    \n",
    "    @delayed\n",
    "    def get_csv(self,csv_path):\n",
    "        \n",
    "        df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
    "        df = df.replace('-', 0)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @delayed\n",
    "    def scaling(self,df,):\n",
    "        \n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
    "            df[col] = df[col] / (self.csv_feature_dict[col][1]-self.csv_feature_dict[col][0])\n",
    "        return df\n",
    "    @delayed\n",
    "    def padding(self,df):\n",
    "        \n",
    "        pad = np.zeros((self.len_of_df, len(df.columns)))\n",
    "        length = min(self.len_of_df, len(df))\n",
    "        pad[-length:] = df.to_numpy()[-length:]\n",
    "        csv = pad.reshape(-1,self.len_of_df,self.feat_len_of_df)\n",
    "        return csv\n",
    "        \n",
    "    @delayed\n",
    "    def get_all_array(self,csv_path_list):\n",
    "        \n",
    "        csvarr = np.empty((0,self.len_of_df,self.feat_len_of_df), float)\n",
    "        for ind,csv_path in enumerate(csv_path_list):\n",
    "                # csv = get_csv(csv_path,csv_feature_dict).compute()\n",
    "                # csv = scaling(csv,csv_feature_dict).compute()\n",
    "                # csv = padding(csv,len_of_df).compute().reshape(-1,len_of_df,feat_len_of_df)\n",
    "                csv = self.get_csv(csv_path)\n",
    "                csv = self.scaling(csv,)\n",
    "                csv = self.padding(csv,)\n",
    "                csvarr = delayed(np.append)(csvarr,csv, axis = 0)\n",
    "        return csvarr\n",
    "    \n",
    "     \n",
    "    @delayed\n",
    "    def getimage(self,imgpath,image_size):\n",
    "        img = cv2.imread(imgpath)\n",
    "        img = cv2.resize(img, dsize=(image_size, image_size), interpolation=cv2.INTER_AREA)\n",
    "        img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
    "        # img = np.transpose(img, (2,0,1))\n",
    "        return img.reshape(-1,image_size,image_size,3)\n",
    "    \n",
    "    @delayed\n",
    "    def get_all_image(self,imgpath_list,image_size):\n",
    "        imgarr = np.empty((0,image_size,image_size,3), float)\n",
    "        for ind,img_path in enumerate(imgpath_list):\n",
    "            img = self.getimage(img_path,image_size)\n",
    "            imgarr = delayed(np.append)(imgarr,img, axis = 0)\n",
    "        return imgarr\n",
    "    \n",
    "    \n",
    "    @delayed\n",
    "    def getlable(self,jsonpath):\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "        crop = json_file['annotations']['crop']\n",
    "        disease = json_file['annotations']['disease']\n",
    "        risk = json_file['annotations']['risk']\n",
    "        label = f'{crop}_{disease}_{risk}'\n",
    "        return label\n",
    "    \n",
    "    \n",
    "    @delayed\n",
    "    def get_label_list(self,labelpath_list):\n",
    "        labelarr = np.array([])\n",
    "        for ind,json_path in enumerate(labelpath_list):\n",
    "            label = label_encoding(self.getlable(json_path))\n",
    "            labelarr = np.append(labelarr,label)\n",
    "        return labelarr\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5759/5759 [00:44<00:00, 128.22it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "# 분석에 사용할 feature 선택\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "csv_files = sorted(glob(path + '\\\\data\\\\train\\\\*\\\\*.csv'))\n",
    "\n",
    "temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
    "max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "\n",
    "# feature 별 최대값, 최솟값 계산\n",
    "for csv in tqdm(csv_files[1:]):\n",
    "    temp_csv = pd.read_csv(csv)[csv_features]\n",
    "    temp_csv = temp_csv.replace('-',np.nan).dropna()#.astype(float)\n",
    "    if len(temp_csv) == 0:\n",
    "        continue\n",
    "    temp_csv = temp_csv.astype(float)\n",
    "    temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "    max_arr = np.max([max_arr,temp_max], axis=0)\n",
    "    min_arr = np.min([min_arr,temp_min], axis=0)\n",
    "\n",
    "# feature 별 최대값, 최솟값 dictionary 생성\n",
    "csv_feature_dict = {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트레인셋경로\n",
    "train_csv_pathlist = sorted(glob(path + '\\\\data\\\\train\\\\*\\\\*.csv'))\n",
    "train_img_list = sorted(glob(path + '\\\\data\\\\train\\\\*\\\\*.jpg'))\n",
    "train_label_list = sorted(glob(path + '\\\\data\\\\train\\\\*\\\\*.json'))\n",
    "#테스트셋경로\n",
    "test_csv_pathlist = sorted(glob(path + '\\\\data\\\\test\\\\*\\\\*.csv'))\n",
    "test_img_list = sorted(glob(path + '\\\\data\\\\test\\\\*\\\\*.jpg'))\n",
    "\n",
    "#CSV 파일셋 설정\n",
    "csvfeature_len = 9\n",
    "csvrows_len = 144\n",
    "#IMG 파일셋 설정\n",
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############트레인##############\n",
    "# trainbatch = 100\n",
    "# #csv 파일셋 로드\n",
    "# controller_train = Controller(train_csv_pathlist[:trainbatch],csv_feature_dict,csvrows_len,csvfeature_len)\n",
    "# train_x1 = controller_train.get_all_array()\n",
    "# #IMG 파일셋 로드\n",
    "# train_x2 = controller_train.get_all_image(train_img_list[:trainbatch],img_size)\n",
    "# #label 파일셋 로드\n",
    "# y_train = controller_train.get_label_list(train_label_list[:trainbatch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############테스트##############\n",
    "# #CSV 파일셋 로드\n",
    "# csvcontroller_test = Controller(test_csv_pathlist,csv_feature_dict,csvrows_len,csvfeature_len)\n",
    "# test_x1 = csvcontroller_test.get_all_array()\n",
    "# #IMG 파일셋 로드\n",
    "# test_x2 = controller_train.get_all_image(test_img_list,img_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eencoder(keras.layers.Layer):\n",
    "    global max_len\n",
    "    def __init__(self):\n",
    "        super(Eencoder, self).__init__()\n",
    "        self.block1_layer1 = tf.keras.layers.Conv1D(9, 24*2, activation='relu',)#input_shape=input_shape[1:])\n",
    "        self.block1_layer2 = tf.keras.layers.Conv1D(18, 24*2+1, activation='relu',)#input_shape=input_shape[1:])\n",
    "        self.block1_layer3 = tf.keras.layers.Conv1D(36, 24*2+1, activation='relu',)#input_shape=input_shape[1:])\n",
    "         \n",
    "    def call(self, inputs):\n",
    "        #LSTM파트\n",
    "        lstm_x = self.block1_layer1(inputs)\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        lstm_x = self.block1_layer2(lstm_x)\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        lstm_x = self.block1_layer3(lstm_x)\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        \n",
    "        return lstm_x\n",
    "class Ddecoder(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Ddecoder, self).__init__()\n",
    "        self.block1_layer1 = tf.keras.layers.Conv1DTranspose(3,  24*2, activation='relu',)#input_shape=input_shape[1:])\n",
    "        self.block1_layer2 = tf.keras.layers.Conv1DTranspose(6,  24*2+1, activation='relu',)#input_shape=input_shape[1:])\n",
    "        self.block1_layer3 = tf.keras.layers.Conv1DTranspose(9,  24*2+1, activation='relu',)#input_shape=input_shape[1:])\n",
    "        \n",
    "         \n",
    "    def call(self, inputs):\n",
    "        #LSTM파트\n",
    "        x = self.block1_layer1(inputs)\n",
    "        \n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.block1_layer2(x)\n",
    "        \n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.block1_layer3(x)\n",
    "       \n",
    "        return x\n",
    "\n",
    "# Eencoder()(x_train[0][:2,:,:]).shape\n",
    "# Ddecoder()(Eencoder()(x_train[0][:2,:,:]))\n",
    "\n",
    "class Autoencoder(tf.keras.Model): \n",
    "  def __init__(self,): \n",
    "    super(Autoencoder, self).__init__() \n",
    "    self.encoder = Eencoder() \n",
    "    self.decoder = Ddecoder() \n",
    "  \n",
    "  def call(self, input): \n",
    "    code = self.encoder(input) \n",
    "    reconstructed = self.decoder(code) \n",
    "    return reconstructed\n",
    "\n",
    "def loss(model, original): \n",
    "  reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(original), original))) \n",
    "  return reconstruction_error\n",
    "\n",
    "def train(loss, model, opt, original): \n",
    "  with tf.GradientTape() as tape: \n",
    "    gradients = tape.gradient(loss(model, original), model.trainable_variables) \n",
    "    gradient_variables = zip(gradients, model.trainable_variables) \n",
    "    opt.apply_gradients(gradient_variables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 10ms/step - loss: 0.2537\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.1345\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0878\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0633\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0544\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0459\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0420\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0341\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0326\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0272\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0244\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0266\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0231\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0238\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0227\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0211\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0200\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0206\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0203\n"
     ]
    }
   ],
   "source": [
    "minibatch_size = 300\n",
    "data_length = len(train_csv_pathlist)\n",
    "loop_num = int(np.floor(data_length/minibatch_size))\n",
    "controller = Controller(csv_feature_dict,csvrows_len,csvfeature_len)\n",
    "\n",
    "automodel = Autoencoder()\n",
    "opt = tf.optimizers.Adam()\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "automodel.compile(optimizer=opt, loss=loss_fn)\n",
    "# automodel.fit(train_x1, train_x1, \n",
    "#                 )\n",
    "count = 0\n",
    "for i in range(loop_num):\n",
    "    \n",
    "    if i != loop_num-1:\n",
    "        #csv 파일셋 로드\n",
    "        train_x1 = controller.get_all_array(train_csv_pathlist[count*minibatch_size:(count+1)*minibatch_size])\n",
    "        #IMG 파일셋 로드\n",
    "        # train_x2 = controller.get_all_image(train_img_list[count*minibatch_size:(count+1)*minibatch_size],img_size)\n",
    "        # #label 파일셋 로드\n",
    "        # y_train = controller.get_label_list(train_label_list[count*minibatch_size:(count+1)*minibatch_size])\n",
    "        \n",
    "        automodel.fit(train_x1.compute().compute(),train_x1.compute().compute())\n",
    "    else:\n",
    "        train_x1 = controller.get_all_array(train_csv_pathlist[count*minibatch_size:])\n",
    "        automodel.fit(train_x1.compute().compute(),train_x1.compute().compute())\n",
    "    \n",
    "    count +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprmodel(keras.Model):\n",
    "    def __init__(self, imgmodel,autoencoder,name = None):\n",
    "        super(preprmodel, self).__init__()\n",
    "        self.imgmodel = imgmodel\n",
    "        self.autoencoder = autoencoder\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x1 = self.imgmodel(inputs[1])\n",
    "        x2 = self.autoencoder.encoder(inputs[0]).numpy().reshape(-1,36)\n",
    "\n",
    "\n",
    "        x = tf.concat([x1,x2],axis=1)\n",
    "        return(x)\n",
    "    \n",
    "model_RESNET50 = ResNet50(weights='imagenet')\n",
    "prepromodel = preprmodel(model_RESNET50,automodel)\n",
    "# x_prime_train = prepromodel(x_train)\n",
    "# x_prime_test = prepromodel(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhqud\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:56:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\", \"xgb_model\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[23:56:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "1\n",
      "[23:57:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\", \"xgb_model\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[23:57:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def return_y(y):\n",
    "    return y.compute()\n",
    "\n",
    "\n",
    "minibatch_size = 200\n",
    "data_length = len(train_csv_pathlist)\n",
    "loop_num = int(np.floor(data_length/minibatch_size))\n",
    "\n",
    "xgb_1 = XGBClassifier(random_state=100, subsample= 0.7, colsample_bytree=0.7, scale_pos_weight=0.2)\n",
    "# for i \n",
    "# xgb.fit(x_prime_train,y_train,xgb_model = xgb)\n",
    "count = 0\n",
    "for i in range(loop_num):\n",
    "# for i in range(1):\n",
    "    \n",
    "    xgb_1 = XGBClassifier(random_state=100, subsample= 0.7, colsample_bytree=0.7, scale_pos_weight=0.2,xgb_model=xgb_1)\n",
    "    print(i)\n",
    "    if i != loop_num-1:\n",
    "        #csv 파일셋 로드\n",
    "        train_x1 = controller.get_all_array(train_csv_pathlist[count*minibatch_size:(count+1)*minibatch_size])\n",
    "        #IMG 파일셋 로드\n",
    "        train_x2 = controller.get_all_image(train_img_list[count*minibatch_size:(count+1)*minibatch_size],img_size)\n",
    "        # #label 파일셋 로드\n",
    "        y_train = controller.get_label_list(train_label_list[count*minibatch_size:(count+1)*minibatch_size])\n",
    "        \n",
    "        xgb_1.fit(prepromodel([train_x1.compute().compute(),train_x2.compute().compute()]),list(map(return_y, y_train.compute())))\n",
    "    else:\n",
    "        #csv 파일셋 로드\n",
    "        train_x1 = controller.get_all_array(train_csv_pathlist[count*minibatch_size:])\n",
    "        #IMG 파일셋 로드\n",
    "        train_x2 = controller.get_all_image(train_img_list[count*minibatch_size:],img_size)\n",
    "        # #label 파일셋 로드\n",
    "        y_train = controller.get_label_list(train_label_list[count*minibatch_size:])\n",
    "        xgb_1.fit(prepromodel([train_x1.compute().compute(),train_x2.compute().compute()]),list(map(return_y, y_train.compute())))\n",
    "        \n",
    "    \n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(path + '\\\\data\\\\sample_submission.csv')\n",
    "# submission['label'] = preds\n",
    "minibatch_size = 200\n",
    "data_length = len(test_csv_pathlist)\n",
    "loop_num = int(np.floor(data_length/minibatch_size))\n",
    "\n",
    "# for i \n",
    "# xgb.fit(x_prime_train,y_train,xgb_model = xgb)\n",
    "count = 0\n",
    "for i in range(loop_num):\n",
    "# for i in range(2):\n",
    "    print(i)\n",
    "    if i != loop_num-1:\n",
    "        start_p = count*minibatch_size\n",
    "        end_p = (count+1)*minibatch_size\n",
    "        #csv 파일셋 로드\n",
    "        test_x1 = controller.get_all_array(test_csv_pathlist[start_p:end_p])\n",
    "        #IMG 파일셋 로드\n",
    "        test_x2 = controller.get_all_image(test_img_list[start_p:end_p],img_size)\n",
    "        \n",
    "        #예측\n",
    "        pred_raw = xgb_1.predict(prepromodel([test_x1.compute().compute(),test_x2.compute().compute()]))\n",
    "        submission.loc[start_p:end_p-1,'label'] = np.array([label_decoder[int(val)] for val in pred_raw])\n",
    "        \n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>6_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>5_b6_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>3_b7_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>3_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>5_b8_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51901</th>\n",
       "      <td>67673</td>\n",
       "      <td>0_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51902</th>\n",
       "      <td>67674</td>\n",
       "      <td>0_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51903</th>\n",
       "      <td>67675</td>\n",
       "      <td>0_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51904</th>\n",
       "      <td>67676</td>\n",
       "      <td>0_00_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51905</th>\n",
       "      <td>67677</td>\n",
       "      <td>0_00_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51906 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image   label\n",
       "0      10000  6_00_0\n",
       "1      10001  5_b6_1\n",
       "2      10002  3_b7_1\n",
       "3      10003  3_00_0\n",
       "4      10004  5_b8_1\n",
       "...      ...     ...\n",
       "51901  67673  0_00_0\n",
       "51902  67674  0_00_0\n",
       "51903  67675  0_00_0\n",
       "51904  67676  0_00_0\n",
       "51905  67677  0_00_0\n",
       "\n",
       "[51906 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('gogo_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8865460eab3b8858828539624ef2f45d875655b94242e338bcb660acf08eeb38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
