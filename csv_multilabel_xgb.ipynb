{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalCluster(3334f920, 'tcp://127.0.0.1:44405', workers=4, threads=16, memory=24.59 GiB) <Client: 'tcp://127.0.0.1:44405' processes=4 threads=16, memory=24.59 GiB>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-oqeisxip', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-1cx41uhr', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-1ihxbx9q', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/gob/repos/Dacon_tabular_img_classification_model/dask-worker-space/worker-dl4k54xi', purging\n"
     ]
    }
   ],
   "source": [
    "# conda install -c rapidsai -c nvidia -c conda-forge dask-cuda cudatoolkit=11.6\n",
    "# nvidia-smi\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import dask.dataframe as dd\n",
    "import dask.multiprocessing\n",
    "import dask\n",
    "import numpy as np\n",
    "from dask import delayed\n",
    "import dask_image.imread\n",
    "import dask_image.ndfilters\n",
    "import dask_image.ndmeasure\n",
    "import cv2\n",
    "from glob import glob\n",
    "import json\n",
    "import dask.array as da\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "from dask import compute\n",
    "from sklearn.metrics import f1_score\n",
    "# f1_score(y_true, y_pred, average=[‘micro’, ‘macro’, ‘samples’,’weighted’ 중 하나 선택])\n",
    "\n",
    "\n",
    "@delayed\n",
    "def padding(data, array_len, col_len):\n",
    "    pad = np.zeros((array_len, col_len))\n",
    "    length = min(array_len, len(data))\n",
    "    \n",
    "    pad[:length] = data[:length]\n",
    "    return pad\n",
    "\n",
    "@delayed\n",
    "def img_resize(img):\n",
    "    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32)/255\n",
    "    return img\n",
    "\n",
    "path = os.getcwd()\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "print(cluster,client)\n",
    "# client.close()\n",
    "# http://localhost:8787/status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making_label_set\n",
    "\n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'0':'정상','1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "\n",
    "#ensemble_labler\n",
    "label_description = {}\n",
    "for key, value in disease.items():\n",
    "    label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
    "    for disease_code in value:\n",
    "        for risk_code in risk:\n",
    "            label = f'{key}_{disease_code}_{risk_code}'\n",
    "            label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
    "\n",
    "global ensemble_label_encoder\n",
    "ensemble_label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "ensemble_label_decoder = {val:key for key, val in ensemble_label_encoder.items()}\n",
    "\n",
    "#crop_labler\n",
    "crop_label_description = {}\n",
    "for key, value in disease.items():\n",
    "    crop_label_description[f'{key}'] = f'{crop[key]}'\n",
    "    \n",
    "global crop_label_encoder\n",
    "crop_label_encoder = {key:idx for idx, key in enumerate(crop_label_description)}\n",
    "crop_label_decoder = {val:key for key, val in crop_label_encoder.items()}\n",
    "\n",
    "\n",
    "#disease_labler\n",
    "disease_label_description = {}\n",
    "for key, value in disease.items():\n",
    "    disease_label_description[f'00'] = f'{crop[key]}'\n",
    "    for disease_code in value:\n",
    "        label = f'{disease_code}'\n",
    "        disease_label_description[label] = f'{disease_code}'\n",
    "\n",
    "global disease_label_encoder\n",
    "disease_label_encoder = {key:idx for idx, key in enumerate(disease_label_description)}\n",
    "disease_label_decoder = {val:key for key, val in disease_label_encoder.items()}\n",
    "\n",
    "\n",
    "#risk_labler\n",
    "risk_label_description = {}\n",
    "for key, value in risk.items():\n",
    "    label = f'{key}'\n",
    "    risk_label_description[label] = f'{key}'\n",
    "\n",
    "global risk_label_encoder\n",
    "risk_label_encoder = {key:idx for idx, key in enumerate(risk_label_description)}\n",
    "risk_label_decoder = {val:key for key, val in risk_label_encoder.items()}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def label_encoding(label):\n",
    "    global ensemble_label_encoder\n",
    "    encoded_label = ensemble_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_crop(label):\n",
    "    global crop_label_encoder\n",
    "    encoded_label = crop_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_disease(label):\n",
    "    global disease_label_encoder\n",
    "    encoded_label = disease_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "@delayed\n",
    "def label_encoding_risk(label):\n",
    "    global risk_label_encoder\n",
    "    encoded_label = risk_label_encoder[label]\n",
    "    return encoded_label\n",
    "\n",
    "def getlable(jsonpath,type):\n",
    "    if type == \"ensemble\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "        crop = json_file['annotations']['crop']\n",
    "        disease = json_file['annotations']['disease']\n",
    "        risk = json_file['annotations']['risk']\n",
    "        label = f'{crop}_{disease}_{risk}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"crop\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        crop = json_file['annotations']['crop']\n",
    "        label = f'{crop}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"dc\":\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        disease = json_file['annotations']['disease']\n",
    "        label = f'{disease}'\n",
    "        return label\n",
    "\n",
    "    elif type == \"risk\":  \n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        risk = json_file['annotations']['risk']\n",
    "        label = f'{risk}'\n",
    "        return label\n",
    "\n",
    "    \n",
    "def get_label_list(labelpath_list):\n",
    "    labelarr = np.array([])\n",
    "    labelarr_crop = np.array([])\n",
    "    labelarr_dc = np.array([])\n",
    "    labelarr_risk = np.array([])\n",
    "\n",
    "\n",
    "    # labelarr = da.array([])\n",
    "    for ind,json_path in enumerate(labelpath_list):\n",
    "        # label = label_encoding(getlable(json_path))\n",
    "        # label = da.array(np.array(label_encoding(getlable(json_path))))\n",
    "        label = np.array(label_encoding(getlable(json_path,\"ensemble\")))\n",
    "        labelarr = np.append(labelarr,label)\n",
    "\n",
    "        label_crop = np.array(label_encoding_crop(getlable(json_path,\"crop\")))\n",
    "        labelarr_crop = np.append(labelarr_crop,label_crop)\n",
    "\n",
    "        label_dc = np.array(label_encoding_disease(getlable(json_path,\"dc\")))\n",
    "        labelarr_dc = np.append(labelarr_dc,label_dc)\n",
    "\n",
    "        label_risk = np.array(label_encoding_risk(getlable(json_path,\"risk\")))\n",
    "        labelarr_risk = np.append(labelarr_risk,label_risk)\n",
    "\n",
    "\n",
    "    return labelarr, labelarr_crop, labelarr_dc, labelarr_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils - ERROR - 'start'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/distributed/utils.py\", line 681, in log_errors\n",
      "    yield\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/distributed/dashboard/components/shared.py\", line 285, in update\n",
      "    ts = metadata[\"keys\"][self.key]\n",
      "KeyError: 'start'\n",
      "tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <zmq.eventloop.ioloop.ZMQIOLoop object at 0x7f12ba384950>>, <Task finished coro=<_needs_document_lock.<locals>._needs_document_lock_wrapper() done, defined at /home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/bokeh/server/session.py:77> exception=KeyError('start')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/tornado/ioloop.py\", line 765, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/bokeh/server/session.py\", line 93, in _needs_document_lock_wrapper\n",
      "    result = func(self, *args, **kwargs)\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/bokeh/server/session.py\", line 227, in with_document_locked\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/bokeh/document/callbacks.py\", line 450, in wrapper\n",
      "    return invoke_with_curdoc(doc, invoke)\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/bokeh/document/callbacks.py\", line 408, in invoke_with_curdoc\n",
      "    return f()\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/bokeh/document/callbacks.py\", line 449, in invoke\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/bokeh/document/callbacks.py\", line 179, in remove_then_invoke\n",
      "    return callback()\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/distributed/dashboard/components/shared.py\", line 306, in <lambda>\n",
      "    self.doc().add_next_tick_callback(lambda: self.update(prof, metadata))\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/bokeh/core/property/validation.py\", line 95, in func\n",
      "    return input_function(*args, **kwargs)\n",
      "  File \"/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/distributed/dashboard/components/shared.py\", line 285, in update\n",
      "    ts = metadata[\"keys\"][self.key]\n",
      "KeyError: 'start'\n",
      "/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/asyncio/base_events.py:711: RuntimeWarning: coroutine 'WSHandler.send_message' was never awaited\n",
      "  handle = events.Handle(callback, args, self, context)\n"
     ]
    }
   ],
   "source": [
    "#making_csv_set\n",
    "row_csv = dd.read_csv(path+\"/train/*/*.csv\",dtype={'외부 누적일사 평균': 'object','내부 이슬점 최고': 'object',\n",
    "       '내부 이슬점 최저': 'object',\n",
    "       '내부 이슬점 평균': 'object'})\n",
    "partitions = row_csv.to_delayed()\n",
    "# datas = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features].values,290,9) for part in partitions]\n",
    "datas = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features][part != \"-\"].dropna().values,290,9) for part in partitions[:]]\n",
    "X_train = np.array(dask.compute(*datas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "멀티라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = sorted(glob(path+\"/train/*/*.json\"))\n",
    "y_train,y_train_crop,y_train_dc,y_train_risk, = get_label_list(label_list)\n",
    "# y_train = y_train.rechunk(5767)\n",
    "results = dask.compute(*y_train)\n",
    "label = np.array(results)\n",
    "# print(label.shape)\n",
    "results_crop = dask.compute(*y_train_crop)\n",
    "label_crop = np.array(results_crop)\n",
    "#\n",
    "results_dc = dask.compute(*y_train_dc)\n",
    "label_dc = np.array(results_dc)\n",
    "#\n",
    "results_risk = dask.compute(*y_train_risk)\n",
    "label_risk = np.array(results_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5767,), (5767,), (5767,), (5767,), (5767, 290, 9))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape,label_crop.shape,label_dc.shape,label_risk.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/xgboost/core.py:502: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  format(\", \".join(args_msg)), FutureWarning\n",
      "/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.547953367233276\n",
      "15.158648490905762\n",
      "15.655789375305176\n",
      "17.159715175628662\n",
      "16.34363031387329\n",
      "16.45718026161194\n",
      "14.51174807548523\n",
      "15.450153827667236\n",
      "14.836581707000732\n"
     ]
    }
   ],
   "source": [
    "params = {\"tree_method\": \"gpu_hist\", \n",
    "          \"objective\": 'multi:softmax',\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.8,\n",
    "        #   \"single_precision_histogram\" : True,\n",
    "          }\n",
    "models = []\n",
    "for i in range(len(csv_features)):\n",
    "  start = time.time()\n",
    "  # globals()[\"XGB_g_{}\".format(i)] = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X_train[:,:,i],label)\n",
    "  _ = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X_train[:,:,i],label)\n",
    "  models.append(_)\n",
    "  # globals()[\"XGB_crop_{}\".format(i)] = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X_train[:,:,i],label_crop)\n",
    "  _ = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X_train[:,:,i],label_crop)\n",
    "  models.append(_)\n",
    "  # globals()[\"XGB_dc_{}\".format(i)] = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X_train[:,:,i],label_dc)\n",
    "  _ = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X_train[:,:,i],label_dc)\n",
    "  models.append(_)\n",
    "  # globals()[\"XGB_risk_{}\".format(i)] = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X_train[:,:,i],label_risk)\n",
    "  _ = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(X_train[:,:,i],label_risk)\n",
    "  models.append(_)\n",
    "  print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5767, 25)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[4].predict_proba(X_train[:,:,i]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_x(dataset,models):\n",
    "    stacked_data = []\n",
    "\n",
    "    for i in range(len(csv_features)):\n",
    "        for j in range(4):\n",
    "            _ = models[i*4 + j].predict_proba(dataset[:,:,i])\n",
    "            stacked_data.append(_)\n",
    "            \n",
    "\n",
    "    # x0=XGB_0.predict(dataset[:,:,0])\n",
    "    # x1=XGB_1.predict(dataset[:,:,1])\n",
    "    # x2=XGB_2.predict(dataset[:,:,2])\n",
    "    # x3=XGB_3.predict(dataset[:,:,3])\n",
    "    # x4=XGB_4.predict(dataset[:,:,4])\n",
    "    # x5=XGB_5.predict(dataset[:,:,5])\n",
    "    # x6=XGB_6.predict(dataset[:,:,6])\n",
    "    # x7=XGB_7.predict(dataset[:,:,7])\n",
    "    # x8=XGB_8.predict(dataset[:,:,8])\n",
    "    # arr_x = np.array([x0,x1,x2,x3,x4,x5,x6,x7,x8,]).T\n",
    "    result = np.concatenate(stacked_data,axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_x = process_x(X_train,models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/xgboost/core.py:502: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  format(\", \".join(args_msg)), FutureWarning\n",
      "/home/gob/anaconda3/envs/dacon_parm_01/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.303757190704346\n"
     ]
    }
   ],
   "source": [
    "params = {\"tree_method\": \"gpu_hist\", \n",
    "          \"objective\": 'multi:softmax',\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\": 0.8,\n",
    "        #   \"single_precision_histogram\" : True,\n",
    "          }\n",
    "\n",
    "start = time.time()\n",
    "XGB_G = xgb.XGBClassifier(params,eval_metric='mlogloss',).fit(processed_x,label)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_path(label):\n",
    "    # csv_path = f'{file}\\\\{file_name}.csv'\n",
    "    # l_path = os.getcwd() + '\\\\data\\\\test\\\\%s\\\\%s.csv'%{label,label}\n",
    "    csv_path = os.getcwd() +f'/test/{label}/{label}.csv'\n",
    "    return csv_path\n",
    "    \n",
    "def img_path(label):\n",
    "    # csv_path = f'{file}\\\\{file_name}.csv'\n",
    "    # l_path = os.getcwd() + '\\\\data\\\\test\\\\%s\\\\%s.csv'%{label,label}\n",
    "    img_path = os.getcwd() +f'/test/{label}/{label}.jpg'\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making_csv_set\n",
    "test_csv = dd.read_csv(path+\"/test/*/*.csv\",dtype={'외부 누적일사 평균': 'object','내부 이슬점 최고': 'object',\n",
    "    '내부 이슬점 최저': 'object',\n",
    "    '내부 이슬점 평균': 'object'})\n",
    "partitions = test_csv.to_delayed()\n",
    "datas_test = [padding(part.drop_duplicates(subset=['측정시각'])[csv_features][part != \"-\"].dropna().values,290,9) for part in partitions[:]]\n",
    "# X_train = np.array(dask.compute(*datas_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(path+\"/sample_submission.csv\")\n",
    "# submission['label'] = preds\n",
    "minibatch_size = 500\n",
    "data_length = len(submission)\n",
    "loop_num = int(np.floor(data_length/minibatch_size))\n",
    "\n",
    "# for i \n",
    "# xgb.fit(x_prime_train,y_train,xgb_model = xgb)\n",
    "count = 0\n",
    "for i in range(loop_num):\n",
    "# for i in range(2):\n",
    "    print(i)\n",
    "    if i !=loop_num-1:\n",
    "        \n",
    "        start_p = count*minibatch_size\n",
    "        end_p = (count+1)*minibatch_size\n",
    "        \n",
    "        #예측\n",
    "        pred_raw = XGB_G.predict(process_x(np.array(dask.compute(*datas_test[start_p:end_p])),models))\n",
    "        submission.loc[start_p:end_p-1,'label'] = np.array([ensemble_label_decoder[int(val)] for val in pred_raw])\n",
    "        \n",
    "    else:\n",
    "        start_p = count*minibatch_size\n",
    "       \n",
    "        #예측\n",
    "        pred_raw = XGB_G.predict(process_x(np.array(dask.compute(*datas_test[start_p:])),models))\n",
    "        submission.loc[start_p:,'label'] = np.array([ensemble_label_decoder[int(val)] for val in pred_raw])\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(path+'/submissions/gogo_submission_07.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0977095985b227be1c62f5d3d4b309e6595c46e2b84a98619f98aa2bfdd71ea2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('dacon_parm_01': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
